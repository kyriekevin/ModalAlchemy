---
title: Data-Parallelism
date: 2025-08-28T00:00:00.000Z
tags:
  - infra
  - megatron
category: note
source: megatron
authors: zyz
---
# Data Parallelism

## 📚 Table of Contents

- [Data Parallelism](#data-parallelism)
  - [📚 Table of Contents](#-table-of-contents)
  - [📝 Summary](#-summary)
  - [🔨 How To](#-how-to)
  - [🎓 Key Points](#-key-points)
  - [💡 Insights](#-insights)

## 📝 Summary

数据并行（DP）是最基础且应用最广泛的分布式训练策略 。其核心思想是在多个工作单元（GPU）上复制完整的模型，然后将全局数据批次（global batch）分割成多个微批次（micro-batch），每个工作单元并行处理一个微批次 。数据并行的主要动机是提升训练吞吐量（即更快地处理数据集）并支持使用更大的全局批量，这有助于提升训练的稳定性 。然而，它存在一个根本性的限制：完整的模型、其优化器状态以及单个微批次的激活值必须能够完全装入单个GPU的内存中 。

---

## 🔨 How To

数据并行的工作流程可以分解为以下五个步骤：

- 步骤1: 模型复制 (Model Replication)
  一个完全相同的模型副本，包括其初始权重和优化器状态，被加载到参与训练的N个GPU上 。

- 步骤2: 数据分片 (Data Sharding)
  全局训练批次被划分为N个更小的微批次。每个GPU接收一个唯一的微批次进行处理 。  

- 步骤3: 并行前向与反向传播 (Parallel Forward & Backward Pass)
  每个GPU独立地对其分配到的微批次执行前向传播以计算损失，随后执行反向传播以计算模型参数的本地梯度 。在此阶段，GPU之间没有通信。  

- 步骤4: 梯度同步 (Gradient Synchronization)
  这是关键的通信步骤。所有N个GPU上计算出的本地梯度需要被聚合。在标准的同步方法中，这一过程通过All-Reduce集合通信操作完成。该操作将所有工作单元的梯度逐元素相加，并将最终的总和结果分发回每个工作单元 。这确保了每个GPU都拥有基于整个全局批次计算出的相同平均梯度。  

- 步骤5: 优化器更新 (Optimizer Step)
  拥有了同步后的梯度，每个GPU执行完全相同的优化器步骤（例如，Adam更新）。由于每个GPU都从相同的权重开始，并接收了相同的平均梯度，它们的模型副本在下一次迭代开始前保持完全同步 。

- PyTorch DDP

```Python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import Adam
dist.init_process_group(backend='nccl')
model = DDP(Model().to(rank), device_ids=[rank])
optimizer = Adam(model.parameters(), lr=lr)

for data, labels in dataloader:
  # 1. & 2. 模型已被复制，数据由分布式采样器分片
  inputs = data.to(rank)
  labels = labels.to(rank)

  # 3. 并行前向与反向传播
  optimizer.zero_grad()
  outputs = model(inputs)
  loss = loss_fn(outputs, labels)
  loss.backward() # 计算本地梯度

  # 4. 梯度同步 (由DDP的loss.backward()自动处理)
  # DDP内部通过hook机制在反向传播后触发All-Reduce

  # 5. 优化器更新
  optimizer.step()  # 所有模型副本被同等地更新
```

---

## 🎓 Key Points

- `All-Reduce` 操作

  - 定义: 一种集合通信原语，其中N个进程各自提供一个输入缓冲区，一个操作（如求和）被逐元素地应用于所有缓冲区，最终结果被分发回所有N个进程 。  

  - 在DP中的作用: 它确保每个模型副本都基于整个全局批次的梯度信息进行更新，而不仅仅是其本地微批次的梯度。这使得分布式数据并行在数学上等效于使用一个非常大的批次在单个GPU上进行训练 。  

  - 实现: 现代深度学习框架（如PyTorch的DDP）使用高度优化的算法（如环形All-Reduce）在高速互联（如NVLink或InfiniBand）上高效地执行此操作，以最小化通信开销 。  

- 同步更新 vs. 异步更新

  - 同步 (标准方法): 所有工作单元必须等待完成其反向传播并参与All-Reduce后，才能进入下一步。这保证了模型的一致性，但可能会被最慢的工作单元（“掉队者”）拖慢 。  

  - 异步: 工作单元无需等待其他节点即可更新中央参数服务器或交换梯度。这可以提高吞吐量，但会引入“梯度陈旧”（stale gradients）问题，即一个工作单元可能基于旧版本的模型权重计算梯度，这可能损害收敛的稳定性 。目前，同步DDP是绝大多数场景下的主流选择。  

- 内存占用
  数据并行不会减少单个GPU上模型本身的内存需求。实际上，模型参数、梯度和优化器状态在每个GPU上都被完整地复制了一份。其主要好处是能够支持更大的全局批量大小。虽然用户可以通过减小单卡微批量大小来为大模型腾出空间，但如果模型本身太大以至于无法在单卡上容纳，数据并行则无能为力 。

---

## 💡 Insights

- DP作为吞吐量扩展器，而非内存节省器
  数据并行的核心机制是模型复制 ，这意味着模型参数和优化器状态的内存负担会随着GPU数量的增加而倍增。它解决的问题不是如何容纳一个大模型，而是如何更快地处理一个大数据集 。通过在批次维度上进行并行化，它直接扩展了训练吞吐量（样本/秒）。这一定位使得数据并行与模型并行策略（如张量并行和流水线并行）有着本质区别，后者的首要目标是降低单个GPU上模型本身的内存占用 。因此，数据并行最好被理解为一种“弱扩展”（weak scaling）策略 ：使用更多的硬件在相同的时间内解决一个更大的问题（处理更多数据）。当模型能放入单个GPU但训练速度需要提升时，数据并行是首选策略。在混合并行方案中，它通常作为基础层存在，但它本身永远不是解决模型过大问题的方案。  

- 通信、批量大小与梯度累积的相互作用
  数据并行的主要开销在于All-Reduce通信 ，这个通信在每个优化器步骤发生一次。梯度累积是一种技术，它在执行一次优化器步骤和相应的All-Reduce之前，在本地累加多个微批次的梯度。通过增加梯度累积的步数，可以在不增加通信频率的情况下增大有效批量大小 ，从而将通信成本摊销到更多的计算上。这揭示了一个关键的调优维度：计算（前向/反向传播）与通信（All-Reduce）的比率。对于给定的全局批量大小，使用更多的GPU（意味着每个GPU的微批量更小）会降低计算通信比，使得All-Reduce成为更显著的瓶颈。反之，使用较少的GPU并配合更多的梯度累积步骤则会提高这个比率。因此，这其中存在一个复杂的权衡。虽然数据并行允许扩展到大量GPU，但其扩展效率受到All-Reduce开销的限制。梯度累积是管理这种权衡的关键工具，它帮助实践者在硬件利用率和通信成本之间找到一个最佳点，尤其是在网络速度较慢的环境中。Megatron-Core等框架中将梯度归约与反向传播过程重叠的功能，是针对这一问题的进一步优化 。
