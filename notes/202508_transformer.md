# Transformer

## 📚 Table of Contents
- [Transformer](#transformer)
  - [📚 Table of Contents](#-table-of-contents)
  - [📝 Summary](#-summary)
  - [🔨 How To](#-how-to)
  - [🎓 Key Points](#-key-points)
    - [第一部分：模型参数量的精确解构](#第一部分模型参数量的精确解构)
      - [嵌入层参数 (Embedding Parameters)](#嵌入层参数-embedding-parameters)
      - [Transformer 核心模块参数 (Per Transformer Block)](#transformer-核心模块参数-per-transformer-block)
      - [最终输出层 (Final Output Layer)](#最终输出层-final-output-layer)
      - [公式汇总 (Final Formula Aggregation)](#公式汇总-final-formula-aggregation)
    - [第二部分：训练显存占用的深度剖析](#第二部分训练显存占用的深度剖析)
      - [静态显存：权重、梯度与优化器状态](#静态显存权重梯度与优化器状态)
      - [动态显存：激活值](#动态显存激活值)
    - [第三部分：训练算力量化](#第三部分训练算力量化)
      - [`6ND` 法则的起源与定义](#6nd-法则的起源与定义)
      - [系数‘6’的推导](#系数6的推导)
      - [总算力计算](#总算力计算)
    - [第四部分：模型规模与数据量的权衡：Chinchilla缩放法则](#第四部分模型规模与数据量的权衡chinchilla缩放法则)
  - [💡 Insights](#-insights)
  - [❓ Questions](#-questions)
  - [🔗 Related Resources](#-related-resources)
  - [🏷️ References](#️-references)

-----

## 📝 Summary

本报告旨在提供一份关于训练标准Transformer架构的大语言模型（LLM）所需GPU显存与计算负载（以FLOPs衡量）的 foundational、理论性解析。分析严格遵循底层数学原理，系统性地推导了模型参数量、显存占用四大核心组成部分（模型权重、梯度、优化器状态、激活值）以及训练总算力的理论计算公式。

为建立一个清晰、明确的理论基准，本报告特意排除了所有现代训练优化技术。报告的核心目的并非提供实践中的最优策略，而是构建一个“第一性原理”的参照系。通过理解这个未经优化的理论基线，能够更深刻地认识到各类优化技术所要解决的核心问题，并能定量地评估其带来的效率提升。

-----

## 🔨 How To

本节提供一个将后续章节中推导的理论公式应用于实践的简明、分步操作指南。

  - **步骤1：计算总模型参数量 ($N$)**

    首先，根据模型的关键超参数，计算其总参数量 $N$。对于一个标准的仅解码器（Decoder-only）Transformer模型，总参数量可以通过以下近似公式快速估算：

    $$
    N \approx v \cdot h + L \cdot (12h^2 + 13h)
    $$

    其中：

      - $v$：词汇表大小（Vocabulary Size）。
      - $h$：模型隐藏层维度（Hidden Dimension），也常被称为 $d\_{model}$。
      - $L$：模型总层数（Number of Layers）。

    此公式的详细推导见“🎓 Key Points”部分。该参数量是后续所有显存和算力计算的基础。

  - **步骤2：计算总训练显存占用 (VRAM)**

    训练过程中的总显存占用是多个部分的加和。其计算公式为：

    $$
    \text{VRAM}*{\text{total}} = \text{Mem}*{\text{weights}} + \text{Mem}*{\text{gradients}} + \text{Mem}*{\text{optimizer}} + \text{Mem}\_{\text{activations}}
    $$

    - **静态显存部分**（权重、梯度、优化器）：对于使用标准AdamW优化器进行混合精度训练的场景，这三部分的总显存占用有一个非常实用的经验法则：每10亿（Billion）模型参数大约需要18 GB的显存。

    $$
    \text{Mem}\_{\text{static}} \approx 18 \cdot N \text{ (bytes)}
    $$

    - **动态显存部分**（激活值）：激活值显存与批次大小（batch size）和序列长度（sequence length）直接相关，需要独立计算。其单层估算公式为：
        $$
        m_{\text{act\_per\_layer}} = b \cdot s \cdot h \cdot \left(34 + \frac{5 \cdot a \cdot s}{h}\right)
        $$

        其中 $b$ 为批次大小，$s$ 为序列长度，$a$ 为注意力头数。总激活显存为该值乘以层数 $L$。

    将静态和动态部分相加，即可得到理论上的总显存需求。

  - **步骤3：计算总训练算力需求 (FLOPs)**

    训练一个模型所需的总计算量（FLOPs, Floating Point Operations）可以通过著名的 `6ND` 法则进行估算。

    $$
    C \approx 6 \cdot N \cdot D
    $$

    - **关键操作与注意事项**：
    \- $C$：训练所需的总FLOPs。
    \- $N$：在此公式中，`N` 特指模型的**非嵌入层参数量**（non-embedding parameters）。在大型模型中，嵌入层参数占比很小，因此通常可以直接使用总参数量进行估算，其误差可以忽略不计。
    \- $D$：训练数据集所包含的总token数（Total number of tokens）。

      - **示例**：训练一个100亿（10B）参数的模型，使用2万亿（2T）token的数据集，所需算力约为 $6 \times (10 \times 10^9) \times (2 \times 10^{12}) = 1.2 \times 10^{23}$ FLOPs。

-----

## 🎓 Key Points

本章节旨在对模型参数、显存占用和算力需求背后的基本原理与公式进行详尽的解构与推导。

### 第一部分：模型参数量的精确解构

一个标准的仅解码器Transformer模型的总参数量 $N$ 主要由三部分构成：嵌入层、 $L$ 个完全相同的Transformer核心模块（Blocks），以及一个最终的输出层。

#### 嵌入层参数 (Embedding Parameters)

1.  **词元嵌入 (Token Embeddings)**：这是一个将离散的词元ID映射为密集向量的查找表。其参数量由词汇表大小 $v$ 和隐藏层维度 $h$ 决定。

    $$
    P_{\text{token\_embed}} = v \cdot h
    $$

    例如，一个拥有50257个词元的词汇表和768维隐藏层的模型（如GPT-2），其词元嵌入参数量为 $50257 \times 768$。

2.  **位置嵌入 (Positional Embeddings)**：用于向模型注入序列位置信息。它同样是一个查找表，其大小由模型能处理的最大序列长度 $p$ (max position) 和隐藏层维度 $h$ 决定。

    $$
    P_{\text{pos\_embed}} = p \cdot h
    $$

    尽管这部分参数对于模型功能至关重要，但在大型模型中，其数量远小于核心模块的参数量，因此在进行高层次的规模估算时常被忽略。

#### Transformer 核心模块参数 (Per Transformer Block)

每个Transformer模块由多头自注意力（Multi-Head Attention, MHA）、前馈网络（Feed-Forward Network, FFN）以及层归一化（Layer Normalization, LN）组成。以下将逐一拆解其参数构成，假设模型隐藏维度为 $h$。

1.  **多头自注意力机制 (MHA)**：
    MHA的参数主要来自四个线性投影层：查询（Query, Q）、键（Key, K）、值（Value, V）的输入投影，以及一个最终的输出投影。在标准的MHA实现中，这四个投影层的输入和输出维度均为 $h$。
      - **Q, K, V 投影**：模型接收维度为 $h$ 的输入，并通过三个独立的线性层分别生成Q, K, V向量。每个线性层包含一个权重矩阵（大小为 $h \\times h$）和一个偏置向量（大小为 $h$）。
        $$
        P_{QKV} = 3 \times (h \cdot h + h) = 3h^2 + 3h
        $$
      - **输出投影**：将所有注意力头的输出拼接后，通过一个线性层将其投影回 $h$ 维度。该层同样包含一个 $h \times h$ 的权重矩阵和一个 $h$ 的偏置向量。
        $$
        P_{\text{output\_proj}} = h \cdot h + h = h^2 + h
        $$
      - **MHA总参数**：
        $$
        P_{\text{MHA}} = (3h^2 + 3h) + (h^2 + h) = 4h^2 + 4h
        $$
2.  **前馈网络 (FFN)**：
    FFN通常由两个线性层和一个非线性激活函数构成。标准设计中，第一个线性层将维度从 $h$ 扩展到一个更大的中间维度 $h_{ff}$，第二个线性层再将其投影回 $h$。
      - **标准假设**：在绝大多数Transformer实现中，中间维度 $h_{ff}$ 被设为隐藏维度的4倍，即 $h_{ff} = 4h$。
      - **第一层 (扩展)**：权重矩阵大小为 $h \times 4h$，偏置向量大小为 $4h$。
        $$
        P_{\text{FFN1}} = (h \cdot 4h) + 4h = 4h^2 + 4h
        $$
      - **第二层 (收缩)**：权重矩阵大小为 $4h \times h$，偏置向量大小为 $h$。
        $$
        P_{\text{FFN2}} = (4h \cdot h) + h = 4h^2 + h
        $$
      - **FFN总参数**：
        $$
        P_{\text{FFN}} = (4h^2 + 4h) + (4h^2 + h) = 8h^2 + 5h
        $$
3.  **层归一化 (Layer Normalization, LN)**：
    每个Transformer模块通常包含两个LN层：一个在MHA之后，另一个在FFN之后。每个LN层都包含两个可学习的参数向量：增益（gamma）和偏置（beta），它们的维度都与输入维度相同，即 $h$。
      - **每个LN层的参数**：$h (\gamma) + h (\beta) = 2h$
      - **模块内LN总参数**：
        $$
        P_{\text{LN}} = 2 \times (2h) = 4h
        $$
4.  **单层模块总参数**：
    将上述三个组件的参数量相加，即可得到单个Transformer模块的总参数量：
    $$
    P_{\text{block}} = P_{\text{MHA}} + P_{\text{FFN}} + P_{\text{LN}} = (4h^2 + 4h) + (8h^2 + 5h) + 4h = 12h^2 + 13h
    $$
    这为“How To”部分给出的估算公式提供了严谨的数学推导。

#### 最终输出层 (Final Output Layer)

模型在经过 $L$ 层Transformer模块后，通常会再经过一个LN层，然后通过一个线性层将最终的隐藏状态投影到词汇表空间，以生成词元概率分布。

  - **最终LN层**：参数量为 $2h$。
  - **输出线性层 (Un-embedding)**：该层的权重矩阵大小为 $h \times v$。在许多现代LLM中，为了节约参数，该层的权重与输入端的词元嵌入权重是 **绑定（tied）** 的，即它们共享同一组参数。因此，这一层通常不引入新的参数。

#### 公式汇总 (Final Formula Aggregation)

综合以上所有部分，一个标准的仅解码器Transformer模型的总参数量 $N$ 的精确计算公式为：

$$
N = (v \cdot h + p \cdot h)_{\text{Embeddings}} + L \cdot (12h^2 + 13h)_{\text{Blocks}} + (2h)_{\text{Final LN}}
$$

对于参数量达到数十亿乃至万亿级别的大型模型而言，$L \cdot 12h^2$ 这一项占据了绝对主导地位，其他线性项（如 $13h$）和嵌入层参数在总参数量中的占比极小，因此在进行规模估算时常常被简化。

### 第二部分：训练显存占用的深度剖析

在训练过程中，GPU显存主要被四类对象占用：模型权重、梯度、优化器状态和激活值。我们将以当前业界标准的混合精度（BF16/FP16）训练作为理论分析的基准场景。

#### 静态显存：权重、梯度与优化器状态

这三部分的大小在训练开始后基本保持不变，仅与模型参数量 $N$ 相关。

在混合精度训练中，为了兼顾训练速度和数值稳定性，不同组件会以不同的精度存储 。具体到每个模型参数，其显存开销可以分解如下：

| 组件 | 精度 | 每参数字节数 | 理由 |
| :--- | :--- | :--- | :--- |
| **模型权重 (活跃)** | BF16/FP16 | 2 | 用于计算密集的前向和反向传播。使用半精度可以利用现代GPU的Tensor Cores进行硬件加速，并减少显存带宽需求。 |
| **模型权重 (主副本)** | FP32 | 4 | 优化器会维护一个FP32精度的权重副本。权重更新发生在这个高精度副本上，以防止因FP16精度不足而丢失微小的梯度更新值（梯度下溢）。 |
| **梯度** | FP32 | 4 | 梯度在计算和累积时始终保持在FP32精度，以维持数值稳定性。因为单个参数的梯度值可能非常小，使用半精度容易导致其被舍入为零。 |
| **优化器状态 (动量)** | FP32 | 4 | Adam/AdamW优化器为每个参数存储一阶矩估计（即梯度的指数移动平均值, $m$）。为保证更新的精确性，该状态以FP32存储。 |
| **优化器状态 (方差)** | FP32 | 4 | Adam/AdamW优化器还为每个参数存储二阶矩估计（即梯度平方的指数移动平均值, $v$）。同样，该状态也以FP32存储。 |
| **总计 (每参数)** | | **18** | **标准、非优化的混合精度训练（使用AdamW）中，每个模型参数所需的总静态显存。** |

从上表可以得出一个至关重要的结论：在静态显存中，**优化器本身是显存消耗大户**。仅Adam(W)优化器的状态（$4+4=8$ 字节）就超过了FP32模型权重（4字节）的显存，并且是FP16活跃权重的四倍。这种不成比例的显存开销是驱动业界研究内存高效型优化器（如8-bit Adam 或 Adam-mini ）的核心动机，这些优化器旨在显著降低每个参数8字节的优化器状态开销。

#### 动态显存：激活值

激活值是在前向传播过程中产生的中间张量。为了在反向传播时计算梯度，这些张量必须被缓存在显存中。因此，激活值显存是纯粹的训练开销，在推理时不存在。其大小与批次大小 $b$ 和序列长度 $s$ 动态相关。

根据文献分析，一个Transformer层产生的激活值显存（以字节为单位）可以表示为：
$$
m_{\text{act\_per\_layer}} = b \cdot s \cdot h \cdot \left(34 + \frac{5 \cdot a \cdot s}{h}\right)
$$

总激活显存为 $m_{\text{act}} = L \cdot m_{\text{act\_per\_layer}}$。这个公式由两部分组成，分别对应线性和二次方两种不同的序列长度扩展模式。

1.  **线性部分解构 ($34 \cdot b \cdot s \cdot h \cdot L$)**

- 该项来源于那些尺寸与 $b \times s \times h$ 成正比的、必须为反向传播而保存的张量。在Transformer模块的前向传播路径中，这些张量包括：
- MHA模块的输入（经过Add & Norm之后）。形状：$[b, s, h]$。
- FFN模块的输入（经过Add & Norm之后）。形状：$[b, s, h]$。
- FFN内部第一个线性层的输出（或激活函数的输入）。形状：$[b, s, 4h]$。
- 以及其他用于残差连接和Dropout的中间结果与掩码。
- 系数 `34` 是一个经验常数，代表了在BF16精度下（每个元素占2字节），将所有这些必需的张量大小加总后的一个综合系数。其精确推导源于论文《Reducing Activation Recomputation in Large Transformer Models》。

2.  **二次项部分解构 ($5 \cdot b \cdot s^2 \cdot a \cdot L$)**

- **来源**：该项完全源于自注意力机制的核心计算。自注意力需要计算序列中 **每个词元对（every token pair）** 之间的相关性得分。
- **张量形状**：对于一个长度为 $s$ 的序列，注意力得分矩阵的形状为 $[s, s]$。考虑到批次大小 $b$ 和注意力头数 $a$，存储这个得分矩阵（或其softmax结果）的完整张量形状为 $[b, a, s, s]$。
- **规模效应**：该张量的大小与序列长度 $s$ 的平方成正比。当 $s$ 较小时，该项可能不显著；但随着 $s$ 的增加，$s^2$ 项的增长速度远超线性项 $s$，使其迅速成为激活值显存乃至总显存中最主要的部分。
- 这种二次方的内存增长是 **在不使用优化技术的情况下，训练长序列Transformer模型的根本理论瓶颈** 。它解释了为什么处理长文本、高分辨率图像或长视频等任务对显存的要求极高。
- 系数 `5` 同样是源于上述论文的经验常数，代表了存储注意力相关张量及其梯度所需的综合开销。

### 第三部分：训练算力量化

量化训练所需的总计算量，通常使用`6ND`法则，这是一个在学术界和工业界被广泛应用的估算标准。

#### `6ND` 法则的起源与定义

该法则是对训练一个Transformer模型所需总浮点运算次数的近似。
$$
C \approx 6 \cdot N \cdot D
$$

- **变量定义**：
- $C$：训练所需的总计算量，单位为FLOPs。
- $N$：模型的**非嵌入层**参数量。如前所述，对于大型模型，使用总参数量替代亦可。
- $D$：训练数据集的总token数，等于 `batch_size * sequence_length * num_training_steps`。
- **出处**：这个简洁而强大的公式由OpenAI在关于模型规模法则（Scaling Laws）的开创性论文中提出并推广。

#### 系数‘6’的推导

系数 `6` 是对每个token在一次完整的前向和反向传播过程中，平均到每个模型参数上所产生的FLOPs的估算。

1.  **前向传播 ($\approx 2N$ FLOPs/token)**

- **权重FLOPs假设**：该推导基于一个核心假设，即模型中绝大多数的计算量来自于包含可学习权重的矩阵乘法（Matrix Multiplications, MatMuls）。
- **单次操作**：一个矩阵乘法中的基本运算是乘加（Multiply-Accumulate, MAC）操作，它包含1次乘法和1次加法，共计2 FLOPs。
- **前向传播计算**：在一次前向传播中，每个输入token流经整个网络。可以近似认为，模型中的**每一个参数**都参与了大约一次MAC运算。因此，处理一个token所需的前向计算量约为 $2 \times N$ FLOPs。此估算忽略了激活函数、层归一化等计算量较小的操作，以及在 $h \gg s$ 时占比很小的注意力得分计算。

2.  **反向传播 ($\approx 4N$ FLOPs/token)**

- **`2x` 规则**：一个被广泛接受的经验法则是，反向传播的计算量约是前向传播的两倍。
- **推导**：这个`2x`的系数源于反向传播的链式法则。对于一个线性层 $Y = XW$，反向传播需要计算两组梯度：
  1.  **对权重的梯度** ($\partial L / \partial W$)，用于更新模型参数。
  2.  **对输入的梯度** ($\partial L / \partial X$)，用于将梯度传播到前一层。
这两组梯度的计算都涉及到与前向传播类似规模的矩阵乘法。因此，总计算量大约是前向传播的两倍。
- **反向传播计算**：
    $$
    C_{\text{backward\_per\_token}} \approx 2 \times C_{\text{forward\_per\_token}} \approx 2 \times (2N) = 4N \text{ FLOPs}
    $$

3.  **总和**：
    将前向和反向传播的计算量相加，得到处理单个token所需的总FLOPs：

    $$
    C_{\text{per\_token}} = (2N)*{\text{forward}} + (4N)*{\text{backward}} = 6N \text{ FLOPs}
    $$

#### 总算力计算

将处理单个token的算力乘以数据集中token的总数 $D$，即可得到训练模型的总算力需求：
$$
C_{\text{total}} = C_{\text{per\_token}} \cdot D = 6ND
$$

### 第四部分：模型规模与数据量的权衡：Chinchilla缩放法则

在拥有了估算总算力需求的`6ND`法则后，一个核心的战略问题随之而来：在固定的总算力预算（C）下，如何最优地分配资源于模型大小（N）和数据量（D）之间，以达到最佳的模型性能（即最低的损失）？

DeepMind在2022年发表的里程碑式论文《Training Compute-Optimal Large Language Models》中对这个问题进行了系统性研究，其结论被称为“Chinchilla缩放法则”。

  - **核心发现**：该研究通过训练超过400个不同规模的模型（参数量从7000万到160亿不等，训练数据量从50亿到5000亿 token不等）发现，为了达到计算最优（Compute-Optimal），模型参数量和训练token数应该 **等比例缩放**。具体而言，**模型大小每翻一倍，训练数据的token数也应相应地翻一倍**。

  - **“20:1” 经验法则**：这一发现催生了一个广为流传的经验法则：对于计算最优的预训练，每个模型参数大约需要**20个训练token**。这意味着，一个拥有10亿（1B）参数的模型，其最优的训练数据量大约是200亿（20B）个token。

  - **对先前模型的修正**：Chinchilla法则指出，在它之前的许多大型模型（如Gopher、GPT-3）相对于其庞大的参数量而言，实际上是“训练不足”（undertrained）的。例如，拥有2800亿参数的Gopher模型仅在3000亿token上进行了训练（约1:1的比例），远低于Chinchilla法则建议的比例。

  - **实证检验**：为了验证这一理论，研究人员使用与训练Gopher（280B参数，300Btoken）大致相同的计算预算，训练了一个更小的、但遵循新缩放法则的模型——**Chinchilla**。该模型拥有700亿（70B）参数，并在1.4万亿（1.4T）token上进行了训练，严格遵守了约20:1的token-参数比。结果显示，尽管Chinchilla的参数量仅为Gopher的四分之一，但它在大量的下游评测任务中一致且显著地优于Gopher、GPT-3（175B）以及Megatron-Turing NLG（530B）等更大的模型。

这一发现标志着大型语言模型研究的范式转变，即从单纯追求更大的模型尺寸，转向更加注重模型规模与数据量之间的优化平衡。

-----

## 💡 Insights

对上述第一性原理的深入剖析，揭示了驱动大型语言模型训练发展的几个核心洞见。

  - **瓶颈分析：静态显存 vs. 动态显存**
    模型训练的显存瓶颈呈现出两种截然不同的模式。对于短序列训练任务，显存占用主要由静态部分决定，即与模型参数量 $N$ 成正比的 `18N` 字节。然而，当序列长度 $s$ 增长时，激活值显存中与 $s^2$ 成正比的二次项会发生爆炸性增长。一旦序列长度超过某个临界点，动态的激活值显存就会超越所有静态部分的总和，成为限制训练规模的首要瓶颈。这一现象深刻地解释了为何长上下文（Long-Context）模型的训练在技术上极具挑战性，并且催生了如Flash Attention等直接针对注意力计算进行优化的关键技术。

  - **从“模型为王”到“数据为王”的范式转移**
    `C ≈ 6ND` 公式在概念上连接了大规模训练的三大支柱：**算力（C）**、**模型大小（N）和数据量（D）**。然而，Chinchilla缩放法则的出现，为如何在这三者之间进行权衡提供了关键的指导，标志着一个重要的范式转变。在此之前，业界普遍认为模型参数量（N）是提升性能的最主要杠杆，导致了对更大模型的盲目追求。Chinchilla通过实证证明，对于一个固定的算力预算（C），许多模型实际上是“训练不足”的。通过将资源从模型参数（N）重新分配到训练数据（D），即用更小的模型学习更多的数据，可以达到更优的性能。Chinchilla模型本身以仅为Gopher四分之一的参数量，在相同的算力预算下取得了更优异的成绩，这雄辩地证明了数据量的关键作用，推动了整个领域从“模型越大越好”转向对数据规模和质量的同等甚至更高程度的重视。

  - **理论计算的价值：理解优化的动机**
    本报告刻意回避了所有优化技术，其目的在于凸显这些理论计算结果所揭示的巨大挑战。从第一性原理推导出的庞大显存和算力需求，为理解整个高效训练领域的研究动机提供了最根本的背景。例如，Adam优化器每个参数高达8字节的状态存储，直接解释了为何业界投入巨大精力研发8-bit优化器或全新的低内存优化算法。激活值内存的 $s^2$ 增长项，则直接阐明了Flash Attention这类技术为何具有革命性意义。因此，这些看似“不切实际”的理论计算，并非纯粹的学术，它们精确地定义了问题本身，是整个高效AI训练技术版图发展的起点和驱动力。

-----

## ❓ Questions

本报告的分析主要基于标准的Transformer架构和既定的缩放法则，这引出了一些值得进一步探索的问题。

  - **Chinchilla法则的普适性与未来**：Chinchilla提出的20:1的token/参数比是基于特定的计算预算和模型架构得出的。然而，现在的模型如Llama Qwen已经采用了远超此比例的“过度训练”（overtraining）策略（例如，超过200:1）并取得了显著成功。这是否意味着Chinchilla法则存在一个“有效期”或适用范围？这种“过度训练”的收益是否存在一个理论上限？模型性能的提升是否会在某个极高的token/参数比后出现饱和甚至下降？

  - **数据质量 vs. 数据数量**：当前的缩放法则主要量化了数据“量”（token总数D）的影响。然而，直觉和一些初步研究表明，数据“质”同样至关重要。一个经过精心筛选和去重的高质量、高信息密度的小型数据集，是否可能让一个较小的模型达到甚至超越一个在庞大但嘈杂数据集上训练的更大模型？我们应如何量化数据质量，并将其作为一个新的变量整合进现有的`C ≈ 6ND`框架中，从而形成一个更全面的缩放法则？

  - **推理成本的权衡**：Chinchilla法则主要优化的是“训练”阶段的计算效率。但在实际应用中，模型的总生命周期成本（TCO）还包括了可能更为庞大的“推理”成本。训练一个更小但“过度训练”的模型，虽然会增加训练成本，但可能会因为模型尺寸更小而大幅降低部署和推理的成本。如何建立一个将训练和推理成本都纳入考量的、面向总生命周期最优的缩放法则？这个最优的token/参数比会如何根据预期的推理负载量而变化？

  - **架构演进对计算量的影响**：`6ND` 法则是基于标准Transformer组件的强大近似。现代架构的变体，如使用SwiGLU激活函数（其FFN层通常有3个权重矩阵而非2个）或分组查询注意力（Grouped-Query Attention），在多大程度上偏离了这个基线？在什么规模或配置下，这些架构创新要求我们对系数 `6` 进行修正？

-----

## 🔗 Related Resources

-----

## 🏷️ References

-----
