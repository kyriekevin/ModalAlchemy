---
title: GRPO-Insight
date: 2025-08-18T00:00:00.000Z
tags:
  - rl
  - grpo
category: insight
authors: zyz
---
# GRPO Insight

## 📚 Table of Contents

- [📝 Context](#-context)
- [💡 Main Thoughts](#-main-thoughts)
- [🔍 Further Questions](#-further-questions)
- [🔗 Related Notes](#-related-notes)
- [🏷️ References](#-references)

## 📝 Context

近年来，LLM的研究焦点已从通用的指令遵循，转向了模型深度的、多步骤的复杂推理能力。以DeepSeek-R1和OpenAI的o系列模型为代表的成果，揭示了基于可验证结果的强化学习（RL with Verifiable Rewards）是解锁这一潜能的关键范式。然而，复现这些顶尖模型的推理能力，远非简单的算法复刻。它是一个复杂的系统工程，要求我们从“食材”（基础模型与数据）、“厨艺”（算法与超参）、“厨房”（基础设施）等多个维度进行整体思考。

## 💡 Main Thoughts

### 理论层面：四大支柱缺一不可

#### 1. 食材决定上限：Base Model and Data Distribution are All You Need

强化学习的核心作用是“发掘并引导模型已有的潜能，而非从零创造”。这一观点在复现高阶推理模型的工作中得到了反复验证。预训练过程为模型注入了海量的、被压缩的结构化知识以及潜在的推理“电路”。强化学习微调（RLHF）的作用，并非教授模型全新的数学公理或逻辑规则，而是激活、精炼这些潜藏的能力，并将它们组织、链接成连贯的、面向目标的思维链条。

实践中观察到的现象——例如，在相同的RLHF流程下，Qwen和Llama系列的基座模型展现出截然不同的性能演化趋势——为这一理论提供了直接的经验证据。DeepSeek团队的DeepSeek-R1-Zero实验，即直接在DeepSeek-V3-Base这样一个纯粹的基座模型上应用RL，并成功涌现出强大的推理能力，更是雄辩地证明了RL作为一种“潜能解锁器”的本质。

这一过程存在明确的因果链条：

1. 强化学习优化的是一个策略（Policy），该策略本质上是模型在现有词汇表和已学成的内部表示（Representations）上的一个概率分布。
2. 如果基座模型在预训练阶段未能接触到足够丰富和高质量的数学、代码或逻辑文本，其内部关于关键概念（如数学符号、逻辑运算符）的表示就会相对薄弱。
3. 当RLHF试图在这些薄弱的表示之上构建复杂的推理策略时，就如同在不坚固的地基上建造高楼，其效果上限被预先决定。
   因此，基座模型的选择是一个具有路径依赖性的关键决策，它在很大程度上预设了后续所有对齐工作的天花板。RLHF更准确的定位是“引导”而非“教导”，它引导模型学会如何结构化地运用其预训练阶段习得的潜在能力，以最大化外部奖励信号。

“垃圾进，垃圾出”的原则在LLM训练中已是共识，但在推理模型的RLHF中，对数据的要求远超于此。数据分布的设计与分析，本身就是一种隐性的课程设计，决定了模型学习的路径和效率。

- **统计丰富性**：对训练数据进行精细的统计分析至关重要。这包括但不限于：每个提示（prompt）在多次采样下的平均准确率、生成回复的平均长度与长度方差、模型不经过思维链（CoT）直接给出答案的比例等。这些指标不仅是数据质量的监控哨，更是衡量数据集多样性、难度分布和信噪比的代理变量，直接关系到训练过程的稳定性。
- **验证器友好性（Verifier-Friendliness）**：对于依赖可验证结果的RL，一个核心要求是奖励信号的准确性和低成本。这意味着数据集的“答案”必须易于被程序化的验证器（Verifier）判断。在数学和代码领域，这通常通过要求模型以特定格式（如将最终答案置于`\boxed{}`中）输出结果来实现，从而允许基于规则的、确定性的验证。这是“RL with verifiable rewards”范式的基石。
- **“食材”与“厨艺”的隐喻**：将数据视为“食材”，将算法和训练技巧视为“厨艺”，是一个非常贴切的类比。许多成功的R1复现工作，其背后往往是对“食材”——即高质量、精心策划的数据集——的极致追求。

#### 2. 规模压倒一切：Scaling is All You Need

在推理模型的奖励机制设计中，长期存在着结果监督（Outcome-supervised Reward Models, ORM）与过程监督（Process-supervised Reward Models, PRM）两条技术路线的争论。将ORM视为“规模化”的代表，将PRM视为“精细化”的代表，有助于我们理解当前SOTA模型的技术选择及其背后的发展逻辑。

- **ORM作为“规模化”路径**：ORM是一种“黑箱”方法，它只关心最终输出结果的正确性，而不对中间的推理过程进行评价。这种方法的优势显而易见：奖励标签的获取成本极低（通常可由验证器自动完成），使其能够轻松扩展至百万甚至千万级别的数据量。然而，其根本缺陷在于“信用分配（Credit Assignment）”难题：一个通过错误、甚至偶然的推理路径（即伪逻辑）得到正确答案的样本，依然会获得正向奖励。这可能导致模型学到并固化错误的推理模式。
- **PRM作为“精细化”路径**：PRM则完全相反，它对推理过程中的每一步都提供细粒度的监督信号。理论上，这能更精确地引导模型学习正确的思考方式。但其代价也极为高昂：首先，过程标注需要大量专家人力，成本高昂；其次，PRM极易遭受Reward Hacking，即模型学会了优化某个局部步骤的奖励得分，但该步骤却对最终的正确答案无益甚至有害。
- **现实的权衡与选择**：DeepSeek-R1等模型的成功，在很大程度上验证了一个阶段性的结论：在当前技术水平下，百万量级的ORM数据所能达成的效果，优于万量级的PRM数据。这并非宣判了PRM的死刑，而是揭示了深度学习发展中一个反复出现的模式——“先规模化，后精细化”。首先利用ORM的可扩展性，“大力出奇迹”，将模型性能推向一个较高的基线；在此之后，当模型已经具备了强大的基础推理能力时，再引入PRM式的精细化方法进行“雕花”，修正推理路径中的细微瑕疵。
- **通往混合模式的未来（HRM）**：学术界已经注意到纯ORM和纯PRM的局限性，并开始探索融合二者优势的混合方法。分层奖励模型（Hierarchical Reward Model, HRM）便是一个代表性的尝试。HRM在评估单个推理步骤（类似PRM）的同时，也考察连续多个步骤构成的“片段”的逻辑一致性。这使得HRM能够识别并奖励“自我修正”行为（例如，一个步骤修正了前一个步骤的错误），同时惩罚那些局部看似正确但全局逻辑不通的推理路径，从而在规模化和精细化之间取得了更好的平衡。

为了更清晰地展示这三种奖励范式的区别，下表进行了系统性对比：

| 特征         | 结果奖励模型 (ORM)                                     | 过程奖励模型 (PRM)                                   | 分层奖励模型 (HRM)                                       |
| ------------ | ------------------------------------------------------ | ---------------------------------------------------- | -------------------------------------------------------- |
| **核心思想** | 仅奖励最终答案的正确性。                               | 奖励每一个中间推理步骤。                             | 在奖励单个步骤的同时，评估多步骤的连贯性。               |
| **粒度**     | 粗粒度（每个序列一个标量奖励）。                       | 细粒度（每个推理步骤一个标量奖励）。                 | 混合粒度（细粒度步骤 + 粗粒度片段）。                    |
| **主要优势** | 可扩展性强，标注成本低，对中间步骤的偶然错误容忍度高。 | 提供直接的正确推理路径信号，信用分配更精确。         | 平衡了可扩展性与精确性，能奖励自我修正，不易被奖励攻击。 |
| **主要劣势** | 信用分配困难，可能奖励伪逻辑（“歪打正着”）。           | 标注成本极高，易于被奖励攻击，会惩罚可被修正的错误。 | 实现与训练比纯ORM/PRM更复杂。                            |
| **数据来源** | 基于规则的验证器，对最终答案的人工标注。               | 专家对推理步骤的逐一标注。                           | PRM数据的超集，包含合并后的连续步骤。                    |

深入分析可以发现，ORM在当前阶段的主导地位，并不仅仅是数据量的胜利，其背后还隐藏着对“探索（Exploration）”的隐性偏好。强化学习需要在探索和利用（Exploitation）之间取得平衡。PRM提供的是密集奖励信号，它严格地将模型引向已知的正确路径，这是一种强烈的“利用”倾向。而ORM提供的是稀疏奖励信号，只要最终答案正确，模型可以自由生成任何中间过程。这种机制极大地鼓励了“探索”，使得模型有机会“偶然”发现人类标注者未曾设想过的、但同样有效甚至更优的全新解题路径。对于复杂的推理任务而言，最优路径往往是未知的或多样的，过于规范的PRM反而可能扼杀模型的创造性。因此，ORM的“缺陷”（信用分配模糊）在某种程度上也是其“优点”（鼓励探索），这或许是其在推理能力训练初期阶段更为有效的原因。

#### 3. 基建是隐形壁垒：Infra is All You Need

当模型规模和响应长度持续增大，“训练效率和预防训练过程中莫名其妙的OOM”成为复现工作的最大难点。可以说，当前各大公司在R1复现进度上的差异，很大程度上反映了其RL基础设施水平的参差不齐。

传统RLHF（如对话、安全对齐）与高阶推理RLHF之间，一个最显著但常被忽视的区别，在于模型生成回复的长度从几百个token急剧增加到数万个token。这一数量级的变化，对训练基础设施提出了根本性的挑战，使其从一个辅助角色，转变为决定项目成败的核心竞争力。

- **序列长度的“暴政”**：在RL的在线采样阶段，为每个prompt生成多条长达数万token的思维链（CoT）回复，会带来巨大的系统开销。
  - **内存与计算开销**：最直接的挑战来自KV缓存。长序列生成过程中，KV缓存会急剧膨胀，消耗大量显存。这不仅拖慢了采样速度，从而拉长了整个训练周期，更极大地增加了训练过程中因显存不足（Out-of-Memory, OOM）而中断的风险。这些OOM往往是“莫名其妙”的，因为它们与动态的生成内容长度紧密相关，难以静态预测。
  - **推理引擎的必要性**：正是在这种背景下，像vLLM和SGLang这样的高效推理引擎，从“锦上添花”变成了“不可或缺”的基础设施组件。它们通过PagedAttention等先进的内存管理技术，实现了对KV缓存的动态、高效调度，使得在有限的显存下生成超长序列成为可能。可以说，没有这类推理引擎的支持，大规模长CoT的RLHF训练几乎无法稳定进行。
- **GRPO：一种“基础设施感知”的算法**：GRPO算法的成功，很大程度上源于其对基础设施限制的深刻理解和巧妙规避。
  - **评论家模型的瓶颈**：在PPO等传统的Actor-Critic框架中，需要维护一个与Actor模型同等规模的Critic模型。这不仅意味着显存占用直接翻倍，还引入了额外的模型加载、同步和管理开销，显著增加了整个训练系统的复杂性和不稳定性。
  - **GRPO的解决方案**：GRPO创造性地用一个简单的在线计算过程取代了需要学习的Critic模型。它通过计算一个采样组内各个样本奖励的相对值（即与组均值和标准差的关系）来估计优势函数（Advantage）。其数学公式为：$A_i​=(r_i​−\frac{mean({r_g​})}{std({r_g​})})$ 。这一设计直接移除了整个Critic模型，极大地节约了显存，简化了训练流程，为应对大模型和长序列带来的内存压力提供了优雅的算法层解决方案。

这种演化揭示了一个深刻的趋势：LLM的RL算法发展，正受到系统工程约束的强烈驱动，其程度不亚于理论本身的创新。GRPO的崛起，是算法为了适应硬件和系统限制而进行“协同设计”的典范。

1. 目标是训练长CoT推理能力。
2. 这在系统层面造成了瓶颈：生成过程中的KV缓存内存占用。
3. 该瓶颈使得传统的、带有Critic的RLHF方法（如PPO）对于超大模型而言变得成本过高或极不稳定。
4. 因此，产生了对资源效率更高算法的“演化压力”。
5. GRPO应运而生，通过移除Critic这一主要内存消耗者，直接在算法层面解决了系统层面的问题。
   这预示着，未来的RL算法不仅要看其理论上的收敛性和有效性，更要评估其“系统感知能力”——即在真实世界的硬件约束下，其资源效率和稳定性如何。

#### 4. 调参是技术自信的体现：Hyper-parameter is All You Need

强化学习算法，尤其是PPO和GRPO，对超参数的敏感性是众所周知 的。学习率、KL散列系数、批次大小、每轮采样的样本数（N）、裁剪比率等，任何一个微小的变动都可能导致训练结果的巨大差异 。在复现高阶推理模型这样前沿且复杂的任务中，这种敏感性被进一步放大。

- **高昂的“确定性成本”**：对于资源有限的团队而言，进行详尽的超参数搜索是一项难以承受的“税负”。他们往往只能进行少数几组实验，一旦结果不理想，很容易过早地得出悲观结论。例如，观察到模型生成了一些逻辑混乱但碰巧答对的样本后，可能会草率地判定ORM这条技术路线本身存在问题，而忽视了这可能仅仅是超参数不当或训练初期的暂时现象。

- **精英团队的优势**：这解释了为何像DeepSeek-R1这样的突破性工作“注定要由精英团队率先做出来”。DeepSeek等顶级研究机构，不仅拥有更深厚的技术积累和自信，更关键的是，他们掌握着海量的计算资源。这种资源优势，使其能够将超参数调优从一个简单的“锦上添花”步骤，提升为核心的“研究与发现”过程。他们可以并行运行数百个消融实验和超参数扫描，系统性地探索广阔的参数空间，从而在大量看似失败的尝试中，找到那条通往稳定收敛的狭窄路径 。

这一现象背后的逻辑链条是：

1. 高阶推理模型的优化目标函数是一个巨大的、非凸的、充满局部最优解的复杂空间。
2. 使用默认或经验性的超参数进行初步实验，其失败或不稳定的概率极高。
3. 资源受限的团队，可能会将这种失败归因于核心方法论的缺陷（例如，“ORM这条路走不通”）。
4. 而拥有大规模算力的团队，则能将同一次失败视为一个更大规模搜索中的一个数据点，通过系统性地改变变量，最终定位到能够稳定训练的参数“甜点区”。 因此，算力在这里不仅仅是加速训练的工具，它从根本上改变了研究范式，使得一种在小规模实验中无法验证的、需要精细调优才能成功的方法论，得以被发现和证实。

### 实验层面：误区规避与进阶探索

#### 常见误区

1. **误区一：R1的关键在于对Base模型做强化**
   一个普遍的误解是，推理能力的涌现来自于直接在原始的基座模型上进行强化学习。然而，对DeepSeek-R1技术报告的深入解读揭示了更为精细的策略 。虽然DeepSeek-R1-Zero确实是从基座模型开始RL，但性能更强的DeepSeek-R1版本，其训练流程始于一个“冷启动（Cold Start）”阶段：先用少量（约1000条）高质量的长思维链数据对基座模型进行监督微调（SFT）。
   这个初始的SFT步骤至关重要，其目的在于为模型“预热”，向其灌输期望的输出格式、基本的推理模式以及遵循指令的能力。这可以有效避免RL在训练初期将大量算力浪费在探索如何从零开始学习这些基础技能上，从而让RL过程能更快地聚焦于优化核心的推理逻辑 。

   - **对Base Model直接做RL**：优点模型的探索空间最大，不受任何先验模式的束缚，理论上有可能发现全新的推理范式；缺点是早期训练不稳定，模型遵循格式的能力很差，易出现语言混杂等问题。
   - **对Long CoT SFT Model做RL**：优点模型已具备良好的格式遵循能力和一定的正确思考模式，RL的起点更高；缺点是可能会有过拟合SFT数据的风险，导致初始生成长度过长或思维模式略显僵化。
   - **对Instruct Model做RL**：这通常是效果最差的选择，因为通用指令微调已经将模型的行为模式固化在一种“乐于助人、简洁明了”的风格上，这与长篇大论、反复思辨的推理模式存在冲突，极大地限制了模型的探索空间。

2. **误区二：R1的复现在于看见Response Length的稳定增长**
   将回复长度的增长视为训练成功的核心指标，是一种典型的“倒果为因” 。正确的逻辑关系是：解决复杂问题需要更高级的推理逻辑，而高级的推理逻辑往往需要更多的文字来承载，从而表现为更长的回复。因此，长度是结果，而非原因。
   我们真正追求的，不是长度本身，而是模型探索到更有价值、更深层次或全新的思考逻辑。如果单纯以增加长度为目标，存在无数种“作弊”方法，如降低`eos_token`的概率、开启`dropout`、增加噪声诱导模型复读等，这些方法只会摧毁模型的推理能力 。
   同理，对于“反思模式（Reflection Pattern）”的出现，如“however”、“wait, let me reconsider”等，也不应简单地将其视为模型具备反思能力的标志。关键在于验证这些反思模式是否真正帮助模型提升了准确率。需要通过数据打点来分析：带有反思词元的回复，其平均准确率是否显著高于不带反思词元的回复？更进一步，不同的反思模式（如“try another approach” vs “compute again”）对准确率的贡献也不同。一个成功的RL过程，应该能观察到模型自发地增加高价值反思模式的出现频率。
3. **误区三：监控指标过少，只关注宏观指标**
   “工欲善其事，必先利其器”。在进行RLHF时，最锋利的“器”就是详尽的日志和监控。鉴于增加监控指标几乎不增加训练时长，原则是“能多尽多”。除了常规的奖励（Reward）、损失（Loss）和回复长度（Response Length），以下维度的细粒度指标对于诊断问题、激发灵感至关重要：

   - **输出同质化指标**：除了策略模型输出的熵（Entropy），还可以计算同一prompt下N条回复之间的编辑距离、N-gram重叠率等，用于监控模型的模式坍塌（Mode Collapse）现象。
   - **条件的回复长度**：分别记录答对/答错时的平均长度，出现/未出现反思时的平均长度，以及这些条件组合下的长度，以量化不同行为模式与资源消耗的关系。
   - **条件的准确率**：不仅要看整体ACC，还要看单条prompt下的ACC（pass@N）、出现特定反思模式时的ACC、回复长度超过平均值时的ACC等，以发现有效的推理策略。
   - **模型异常行为占比**：统计不遵循格式、超长、复读（可用重复分数衡量）、中英混杂等异常输出的比例，作为模型健康度的度量。
   - **算法内部状态指标**：GRPO有一个核心的裁剪（Clip）操作。记录裁剪发生的频率、是上溢出还是下溢出居多、以及裁剪比例随训练的演变趋势，这对于调试RL算法本身至关重要。

4. **误区四：在细枝末节上迷失，忘记核心目标**
   在复杂的RL实验中，我们时常会陷入为了优化某个次要指标而损害核心目标的“死胡同”。一个典型的例子就是过度执着于“维持模型输出的多样性（高熵）”。
   这种执念的逻辑链通常是：希望熵不要太低 -> 因为希望N条回复有区分度 -> 于是增加熵损失（entropy_loss）、调高温度（temperature） -> 结果模型被训崩溃 -> 陷入困惑。
   这里的根本矛盾在于，维持高熵（探索）与语言模型本身的优化目标（利用、收敛）在一定程度上是冲突的。RLHF的核心目标之一，正是要将模型从“采样10次才能答对1次”训练到“采样3次就能答对1次”，这是一个将`pass@N`的收益向`pass@1`集中的过程，本身就伴随着熵的自然下降。
   正确的态度应该是：只要核心目标——奖励（准确率）稳定提升，且模型探索出了一些有价值的高级思考模式——在持续改善，就无需过度纠结于输出多样性是否下降。如果多样性确实成为瓶颈（例如，模型对所有难题都只会一种无效的尝试），那么问题根源更可能在数据端（prompt太难或太简单），此时应考虑引入课程学习等策略调整数据分布，而不是在损失函数上“走火入魔”。

#### 进阶策略探索

基于DAPO等算法的核心机制——即对探索（熵）与利用（优势信号）的平衡调控——我们可以设计一系列更动态、更智能的数据驱动训练策略，将训练从静态规则转变为一个智能过程。

1. **课程学习（Curriculum Learning）**

   - **核心思想**：摒弃对整个数据集的随机采样，通过“由易到难”的课程安排，让模型循序渐进地构建推理能力，从而构建更扎实的推理能力 。  
   - **单维度排序**：最直接的方式是根据模型在各个问题（prompt）上的历史**通过率**进行排序。训练初期，优先选择通过率高（简单）的样本，让模型快速掌握基础模式。
   - **多维度排序与“矛盾”样本**：更精细化的方法是结合**通过率**和**方差**。这里的方差可以理解为：对于一个包含多个子维度或解题步骤的问题，如果模型在这些子维度上的得分高低不齐，说明这是一个“矛盾”的样本，其标准差会较大。反之，如果得分一致（全对或全错），标准差则为零。课程可以设计为从“信号一致”（低方差）的样本过渡到“信号矛盾”（高方差）的样本，引导模型逐步解决更具挑战性的问题。

2. **作弊采样（Cheat Sampling）**

   - **核心思想**：直接解决模型在难题上的“冷启动”困境。当模型自身的探索能力完全无法生成一个正确答案时（即某prompt的通过率为0），训练信号便会消失。
   - **机制**：当一个采样组的通过率判定为0时，强制性地将一个由专家或更强模型生成的标准答案（Ground Truth, GT）注入到该采样组中。
   - **双重收益**：
     1. **提供学习信号**：这保证了组内至少存在一个正奖励样本，从而产生一个宝贵的、非零的优势信号，为模型在难题上的梯度更新指明方向。
     2. **提升探索熵**：对于当前策略模型而言，难题的GT解路径通常是其概率分布下的低概率事件，即具有高熵。注入这样的高熵样本，特别是在课程学习的后期，可以有效对抗因过拟合简单模式而导致的熵坍塌，从而增加模型在困难问题上的探索能力。

3. **SSR机制（Significant Sample Replay）**
   - **核心思想**：这是一种针对RLHF场景的优先经验回放（Prioritized Experience Replay）变体，旨在最大化高价值样本的利用效率。
   - **机制**：维护一个存放“关键样本”的缓冲区。当在线采样过程产生了一个低价值的样本组（例如，所有采样都错误且模式单一，标准差为0）时，用一个从缓冲区中取出的高价值样本组来替换它，进行梯度更新。
   - **“关键”的定义**：高价值样本是那些能揭示模型能力变化或缺陷的样本。例如：“难题做对”（表明模型探索到了新的有效路径）、“简单题做错”（表明模型在基础能力上存在漏洞或发生了灾难性遗忘）。通过“作弊采样”注入的GT样本组，天然就是一种高价值的关键样本。通过回放这些关键事件，可以强化模型对核心知识点和新习得技能的记忆与巩固，从而实现更高效的优化。

## 🔍 Further Questions

基于上述理论思考和实验探索，未来值得深入研究的方向包括：

- **理论层面**：

  - 如何量化并预测一个基础模型的“推理潜力”？是否存在可解释性的方法来识别模型内部的“推理电路”？
  - ORM、PRM与HRM的平衡点在何处？是否存在一个理论框架，可以根据任务复杂度和可用数据量，指导我们选择最优的奖励建模策略，甚至动态地在两者之间切换？
  - 能否将“基础设施感知”形式化为一个算法设计原则？即在算法设计之初就将内存占用、计算延迟等硬件约束作为优化目标之一。

- **实验层面**：
  - **课程学习的理论边界**：
    - **最优难度定义**：如何从理论上定义一个对模型最优的“问题难度”？它仅仅是历史通过率的函数，还是一个包含了问题复杂度、解题路径方差、所需知识领域等多维度的向量？
    - **避免局部最优**：动态课程是否存在陷入“能力陷阱”的风险？即模型为了最大化短期学习增益（Advantage），持续在某一类“中等难度”问题上进行训练，而回避了那些虽然短期学习信号弱但对长期能力发展至关重要的难题。
    - **收敛性保证**：在非平稳的课程学习设置下（模型和数据分布都在变化），当前RL算法的收敛性保证是否依然成立？
  - **数据注入的泛化与风险**：
    - **最优注入率**：是否存在一个最优的“作弊采样”注入率？过低的注入率可能无法有效引导模型，而过高的注入率是否会压制模型的自主探索，导致其只会“模仿”标准答案的路径，从而损害泛化能力？
    - **注入数据的多样性**：注入的GT样本是否应该具有多样性？如果一个难题有多种解法，是应该只注入一种最优解，还是应该注入多种不同风格的解法以鼓励模型探索更广阔的解空间？
    - **负面教学**：除了注入“正确”答案，是否可以策略性地注入“典型错误”的样本，并给予明确的负向信号，以帮助模型更快地学会避开常见的推理陷阱？
  - **样本回放的效率与平衡**：
    - **“关键性”的动态度量**：如何动态地、自适应地评估一个样本的“关键性”？一个在训练初期被认为是“关键”的样本（如“难题做对”），在训练后期是否仍然关键？“关键性”的度量应该如何随模型能力的演进而变化？
    - **回放与在线采样的平衡**：SSR机制中的回放样本与实时在线采样的样本之间，最佳的混合比例是多少？过度依赖回放可能导致模型对旧数据过拟合，而回放不足则可能导致对关键知识的灾难性遗忘。
    - **跨任务回放**：在多任务学习场景下，一个任务中的“关键样本”是否可以被回放到另一个相关任务的训练中，以促进知识迁移和泛化？

## 🔗 Related Notes

- [dapo](../papers/llm/train/rl/20250318_dapo.md)

## 🏷️ References

- [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2501.12948)
- [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
- [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/abs/2505.14970)
- [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/abs/2503.13551)
- [DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization](https://arxiv.org/abs/2503.14476)
- [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
