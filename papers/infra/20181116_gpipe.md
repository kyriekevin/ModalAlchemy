---
title: 'GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism'
date: 2025-09-01T00:00:00.000Z
tags:
  - infra
  - pipeline-parallelism
category: paper
venue: arXiv NeurIPS
authors: zyz
---
# Gpipe

## 📚 Table of Contents
- [Gpipe](#gpipe)
  - [📚 Table of Contents](#-table-of-contents)
  - [📖 Paper Info](#-paper-info)
  - [📝 Summary](#-summary)
  - [🔑 Key Contributions](#-key-contributions)
  - [🧩 Method](#-method)
    - [朴素模型并行](#朴素模型并行)
    - [GPipe 流水线并行](#gpipe-流水线并行)
      - [`micro-batch`](#micro-batch)
      - [re-materialization](#re-materialization)
  - [📊 Experiments](#-experiments)
  - [💬 Personal Insights](#-personal-insights)
  - [🔗 Related Papers](#-related-papers)

---

## 📖 Paper Info

- **Title:** GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
- **Authors:** Google
- **Venue:** arXiv NeurIPS
- **Paper Link:** <https://arxiv.org/abs/1811.06965>
- **Publication Date:** 2018-11-16

---

## 📝 Summary

本文将探讨流水线并行，经典的流水线并行范式有Google推出的`Gpipe`和微软提出的`PipeDream`，两者主要差别是在梯度更新上`Gpipe`是同步的，`PipeDream`是异步的。`Gpipe`因为性能和浅显易懂更受欢迎（torch的pp接口就是基于Gpipe）。

核心思想是批次拆分流水线算法（batch-splitting pipelining algorithm）。该算法首先将模型的网络层序列切分到多个加速器上，然后将一个训练小批量（mini-batch）数据进一步拆分为多个微批次（micro-batches），并让这些微批次在不同加速器的模型分区之间进行流水线式处理，从而实现了高度并行的计算。

该方法在训练吞吐量上实现了近乎线性的加速比,同时保证了梯度更新在数学上与传统的顺序训练完全一致，确保了模型训练的可复现性和收敛质量 。

---

## 🔑 Key Contributions

- `micro-batch`
  在朴素模型并行中，由于计算的顺序依赖性而导致的加速器空闲时间 。通过允许不同加速器同时处理不同的微批次，GPipe显著提升了硬件的整体利用率 。论文的实验证明，这种方法带来了近乎线性的训练加速比。

- `re-materialization`
  GPipe采用 **同步小批量梯度下降（synchronous mini-batch gradient descent）** 策略。在一个小批量（mini-batch）中，所有微批次（micro-batches）计算出的梯度会被累积起来，然后在所有微批次都完成后，进行一次统一的、全局同步的权重更新 。这种机制确保了无论模型被分割成多少个分区（K），其训练动态都与标准的单加速器训练完全相同 。

---

## 🧩 Method

### 朴素模型并行

当单卡装不下模型时，一个直接的解决方法是：把模型切分成不同层，每个部分都放在不同GPU上
![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011209007.png)

此时模型的`forward`和`backward`的过程
![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011316888.png)

> 其中下标表示batch编号，这里只有一个batch，因此下标都是0。每一行表示一个GPU，每一列表示timestep

这张图片含义是：在GPU0上做完一次`forward`，然后将GPU0上的输出传给GPU1继续`forward`，直到四块GPU都`forward`完成，再依次`backward`。等所有`backward`完成后，最后一时刻统一更新梯度。

存在两个问题：

1. GPU利用不够
   白色部分所表示的时间段里，总有GPU在空转，这些被定义为`bubble`。
   假设有`K`块GPU，每块GPU进行`forward` `backward`时间为 $t_{fb} = (t_f + t_b)$

   - 整体时间的面积：$K * Kt_{fb}$
   - `forward` `backward`时间面积：$Kt_{fb}$
   - `bubble`时间面积：$K * Kt_{fb} - Kt_{fb} = (K - 1)Kt_{fb}$
   - `bubble`时间占比：$(K - 1)Kt_{fb}/K Kt_{fb} = \frac{K-1}{K}$

   `bubble`时间复杂度为$O(\frac{K-1}{K})$，当K越大，即GPU的数量越多时，空置的比例接近1，即GPU的资源都被浪费掉了。

2. 中间结果占据显存
   ![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011336932.png)

   在做`backward`计算梯度的过程中，我们需要用到每一层的中间结果`z`.假设模型有`L`层，每一层的宽度为`d`,则对于每块GPU，不考虑其参数本身的存储，额外的空间复杂度为$O(N * \frac{L}{K}d)$。从这个复杂度可以看出，随着模型的增大，N，L，d三者的增加可能会平滑掉K增加带来的GPU内存收益。

### GPipe 流水线并行

流水线并行的核心思想是：在模型并行的基础上，进一步引入数据并行的办法，即把原先的数据再划分成若干个batch，送入GPU进行训练。未划分前的数据，叫`mini-batch`。在`mini-batch`上再划分的数据，叫`micro-batch`。

#### `micro-batch`

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011354018.png)

> 第一个下标表示GPU编号，第二个下标表示`micro-batch`编号。

- 前向传播（Forward Pass）
  一个大小为 N 的小批量（mini-batch）被切分为 M 个微批次（micro-batches）。第一个加速器（阶段0）处理完第一个微批次后，将其输出的激活值传递给第二个加速器（阶段1）。在阶段1开始处理第一个微批次的同时，阶段0立即开始处理第二个微批次。这种流水线式的执行方式持续进行，直到所有微批次都通过了全部 K 个阶段 。

- 反向传播（Backward Pass）
  反向传播以类似但相反的顺序进行。当最后一个微批次在最后一个阶段完成前向传播后，该阶段立即计算梯度，并将梯度传递回前一个阶段。重要的是，每个阶段在为某个微批次计算梯度时，使用的是与其前向传播时完全相同的模型参数 。

- 同步权重更新（Synchronous Weight Update）
  在每个阶段，各个微批次的梯度在本地被累积。当所有 M 个微批次都完成了前向和反向传播后，系统会执行一次全局的、同步的`all-reduce`操作来聚合所有加速器上的梯度。随后，模型权重在每个小批次（mini-batch）结束时统一更新一次 。

- 流水线气泡（The Pipeline Bubble）
  这种调度方式不可避免地会在每个小批次的开始（填充阶段）和结束（排空阶段）产生一段加速器空闲时间，这部分空闲时间就是“流水线气泡”开销 。论文指出，这个气泡的开销可以被微批次的数量 M 摊销。气泡时间占总时间的比例近似为 (K−1)/(M+K−1) 。这个公式很好地解释了实验中的一个重要发现：当 M 远大于 K 时（例如，实验中当 M≥4K 时），气泡开销几乎可以忽略不计 。

#### re-materialization

随着模型的增加，每块GPU中存储的中间结果也会越大。对此，Gpipe采用了一种非常简单粗暴但有效的办法：用时间换空间，在论文里，这种方法被命名为re-materalization，后人也称其为active checkpoint。
具体来说，就是几乎不存中间结果，等到backward的时候，再重新算一遍forward

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011400121.png)

每块GPU上，我们只保存来自上一块的最后一层输入z，其余的中间结果我们算完就废。等到backward的时候再由保存下来的z重新进行forward来算出。

现在我们来计算每块GPU峰值时刻的内存：
每块GPU峰值时刻存储大小 = 每块GPU上的输入数据大小 + 每块GPU在forward过程中的中间结果大小

每块GPU上固定需要保存它的起始输入，我们记起始输入为 $N$ （即mini-batch的大小）。每个micro-batch是流水线形式进来的，算完一个micro-batch才算下一个。在计算一个micro-batch的过程中，我们会产生中间变量，它的大小为 $\frac{N}{M} * \frac{L}{K} * d$ 其中M为micro-batch个数）。
因此，每块GPU峰值时刻的空间复杂度为 $O(N + \frac{N}{M} * \frac{L}{K} * d)$
将其与朴素模型并行中的GPU空间复杂度 $O(N * \frac{L}{K} * d)$ 比较，可以发现，由于采用了micro-batch的方法，当L变大时，流水线并行相比于朴素模型并行，对GPU内存的压力显著减小。

---

## 📊 Experiments

- 实验设置
  - 模型： 实验选用了两种具有代表性的不同架构：用于图像分类的AmoebaNet（一种卷积网络）和用于多语言神经机器翻译的Transformer（一种基于注意力机制的序列模型）。
  - 数据集： 分别使用了ImageNet 2012数据集进行图像分类，以及一个包含103种语言、250亿训练样本的大规模内部多语言语料库进行机器翻译 。
  - 硬件： 实验在Google的Cloud TPUv2和TPUv3上进行，同时也在没有高速NVLink互连的NVIDIA P100 GPU集群上进行了测试，以评估其在通信受限环境下的性能 。

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011413042.png)

- 在Transformer上，Gpipe基本实现了模型大小（参数量）和GPU个数之间的线性关系。例如从32卡增到128卡时，模型的大小也从21.08B增加到82.9B，约扩4倍
- 对AmoebaNet而言，却没有完全实现线性增长。例如从4卡到8卡，模型大小从1.05B到1.8B，不满足2倍的关系。本质原因是AmoebaNet模型在切割时，没有办法像Transformer一样切得匀称，保证每一块GPU上的内存使用率是差不多的。因此对于AmoebaNet，当GPU个数上升时，某一块GPU可能成为木桶的短板。

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011416268.png)

M=32表示micro-batch的数量为32，K表示GPU数量。从实验结果可知，在关掉NVlinks的情况下，Gpipe一样也能实现随着GPU数量的增加，训练速度也增加的效果。虽然这两者间不是线性的。同样，因为模型切割不均的原因，AmoebaNet的表现不如Transformer。

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202509011415624.png)

当重新开启NVlinks后，我们来看M的大小（即流水线的核心）对训练速度的影响。

- 当M=1的时候，如前文所说，GPU的空置率太高，因此两个模型都没有实现训练速度和GPU个数间的线性关系
- 当M=4时，表现明显好转。
- 当M=32时，表现最佳，且Transformer基本实现了训练速度和GPU个数的线性关系。

---

## 💬 Personal Insights

一个核心的观点是，数据并行、张量并行和流水线并行并非相互竞争的技术，而是解决不同系统瓶颈的互补工具。数据并行扩展了每步处理的数据量，而张量和流水线并行则扩展了模型本身的大小。现代的大规模训练系统几乎无一例外地采用了结合多种策略的混合并行方法 。

数据并行通过复制模型来并行处理数据分片，但无法解决单模型过大的问题 。张量并行通过水平切分单个算子（如矩阵乘法）来解决单层过大的问题，但通信开销极高 。GPipe所代表的流水线并行则通过垂直切分模型层来解决模型深度过大的问题，通信开销较低，但引入了流水线气泡 。因此，一个典型的现代大模型训练方案可能是：使用张量并行来处理单个巨大的Transformer层，使用流水线并行将数十上百个这样的层串联起来分布在不同节点上，最后再使用数据并行复制整个流水线，以增加全局批处理大小，加速收敛。GPipe的贡献正是这个“3D并行”中的关键一维。

GPipe并非流水线并行的终点，而是定义了该问题空间和基准解决方案的开创性工作。它所暴露出的核心局限——流水线气泡——成为了后续一系列系统研究的中心议题。

1. GPipe (同步流水线): 提出了基准方案，简单、正确，但存在气泡 。
2. PipeDream (异步流水线): 为了消除气泡，采用了异步方法，不再等待流水线排空，从而获得了更高的硬件吞吐量 。但这引入了 **梯度延迟（stale weights）** 的新问题，即梯度是基于旧的模型权重计算的，可能影响模型的收敛性，这是一种典型的用统计效率换取硬件效率的权衡 。
3. PipeDream-2BW, WPipe (优化的同步流水线): 后续研究试图寻求“两全其美”的方案。它们回归到同步或近同步的更新模式以避免梯度延迟，同时采用更复杂的调度和内存管理技术（如权重双缓冲）来显著减小与GPipe相比的气泡大小和内存占用 。其中，PipeDream-Flush作为PipeDream-2BW的一个变体，在机制上被认为与GPipe基本相同，但采用了更优的内存调度策略 。

---

## 🔗 Related Papers
