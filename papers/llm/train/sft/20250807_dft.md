---
title: >-
  On the Generalization of SFT: A Reinforcement Learning Perspective with Reward
  Rectification
date: 2025-08-18T00:00:00.000Z
tags:
  - sft
category: paper
venue: arXiv
authors: zyz
---
# DFT

## 📚 Table of Contents

- [📖 Paper Info](#-paper-info)
- [📝 Summary](#-summary)
- [🔑 Key Contributions](#-key-contributions)
- [🧩 Method](#-method)
- [📊 Experiments](#-experiments)
- [💬 Personal Insights](#-personal-insights)
- [🔗 Related Papers](#-related-papers)

---

## 📖 Paper Info

- **Title:** On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification
- **Venue:** arXiv preprint
- **Paper Link:** [https://arxiv.org/abs/2508.05629](https://arxiv.org/abs/2508.05629)
- **Publication Date:** 2025-08-07
- **Code:**([https://github.com/yongliang-wu/DFT](https://github.com/yongliang-wu/DFT))

---

## 📝 Summary

本文提出了一种名为动态微调（Dynamic Fine-Tuning, DFT）的方法，旨在解决大型语言模型LLM在监督微调SFT过程中泛化能力受限的问题。尽管SFT因其简单高效而被广泛应用，但其性能通常不及更为复杂的强化学习（RL）方法 。通过数学分析，将标准SFT的梯度更新过程重新诠释为一种特殊的离线策略梯度（offline policy gradient）算法。该分析揭示了SFT中隐含着一个有问题的奖励结构：其奖励值与模型对专家行为（expert action）的预测概率成反比。这种“反向概率加权”会导致梯度方差无界，从而引发训练不稳定和泛化能力差等问题。

为纠正这一缺陷，DFT方法被提出。它通过一个简单的修改——在计算损失时，将每个词元（token）的损失项乘以该词元的预测概率——来动态地重新缩放SFT的目标函数。这一操作从根本上“修正”了SFT内在的病态奖励结构，使其转变为一个对所有专家行为都给予恒定奖励的、更稳健的优化目标

---

## 🔑 Key Contributions

- **SFT与RL的理论统一**：建立了标准SFT梯度与离线策略梯度之间的数学等价关系。以往的研究虽然指出了两者间的普遍联系，但未能揭示其内在机制。指出SFT的泛化瓶颈源于其梯度中一个隐式的“反向概率加权项”（即奖励与 $1/\pi_{\theta}$ 成正比），这个项导致了稀疏且病态的奖励结构，是训练不稳定的根源。

- **提出动态微调（DFT）方法**：提出了一种名为动态微调（DFT）的创新方法，旨在从根本上修正SFT的内在缺陷。DFT通过将每个词元的损失函数乘以其自身的预测概率，有效地抵消了前述的病态加权项，从而将隐式奖励修正为一个恒定为1的、行为良好的信号。

---

## 🧩 Method

### 理论之桥：将SFT重新诠释为策略梯度方法

标准的SFT旨在通过最大化专家演示数据集 $D = \{(x, y^*)\}$ 的似然来进行模型训练，其目标是模仿专家的行为。这通常通过最小化负对数似然（Negative Log-Likelihood, NLL）来实现，目标函数 $L_{SFT}(\theta)$ 定义如下：
$$L_{SFT}(\theta) = - \mathbb{E}_{(x, y^*) \sim D} \left[ \sum_{i=1}^{|y^*|} \log P_{\theta}(y_i^* | x, y_{<i}^*) \right]$$
其中，$y_i^*$ 是专家序列中的第 $i$ 个词元，$y_{<i}^*$ 是其前面的词元序列。为了简化表示，我们将模型 $P_{\theta}$ 视为策略 $\pi_{\theta}$，将专家行为 $y_i^*$ 视为动作 $a_i$，将上下文 $(x, y_{<i}^*)$ 视为状态 $s_i$。于是，SFT的目标函数可以重写为：
$$L_{SFT}(\theta) = - \mathbb{E}_{(s_i, a_i) \sim D} [\log \pi_{\theta}(a_i | s_i)]$$为了最小化这个损失函数，我们使用梯度下降法，需要计算其关于模型参数 $\theta$ 的梯度：$$\nabla_{\theta} L_{SFT}(\theta) = - \mathbb{E}_{(s_i, a_i) \sim D} [\nabla_{\theta} \log \pi_{\theta}(a_i | s_i)]$$
这个梯度形式与强化学习中的策略梯度（Policy Gradient, PG）定理在结构上高度相似。策略梯度定理指出，对于一个最大化期望奖励 $J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}$ 的目标，其梯度为：
$$\nabla_{\theta} J(\theta) = \mathbb{E}_{(s_t, a_t) \sim \pi_{\theta}}$$
本文的核心洞见在于，SFT的梯度更新可以被精确地解释为一个具有特定隐式奖励 $R_{SFT}$ 的离线策略梯度更新。通过将SFT的梯度形式与策略梯度定理进行对齐，论文揭示了这个隐式奖励被精确地定义为：
$$R_{SFT}(s_i, a_i) = \frac{1}{\pi_{\theta}(a_i | s_i)}$$
这个奖励结构存在两个致命缺陷：

1. **反比关系**：奖励的大小与模型对专家行为的预测概率成反比。这意味着，当模型对某个词元的预测越不确定（即 $\pi_{\theta}$ 越小），一旦预测正确，它获得的奖励信号反而会呈指数级增长。
2. **方差无界**：当 $\pi_{\theta}(a_i | s_i) \to 0$ 时，$R_{SFT} \to \infty$。当模型对某个专家词元的预测概率$\pi_\theta$非常低时（即模型对这一步感到“意外”或“不确定”），其对应的隐性奖励会变得极其巨大。这会导致梯度更新的方差趋于无穷大。在训练过程中，模型会为了拟合这些低概率的“异常点”而进行剧烈且不稳定的参数调整，从而破坏了学习过程的平稳性。

### 奖励修正：DFT的解决方案及其机制

为了解决上述问题，DFT对标准SFT的损失函数引入了一个动态缩放因子。DFT的损失函数 $L_{DFT}(\theta)$ 定义如下：
$$L_{DFT}(\theta) = - \mathbb{E}_{(x, y^*) \sim D} \left[ \sum_{i=1}^{|y^*|} \pi_{\theta}(y_i^* | x, y_{<i}^*) \cdot \log P_{\theta}(y_i^* | x, y_{<i}^*) \right]$$
在实际计算中，用于缩放的概率项 $\pi_{\theta}$ 是从计算图中分离（detach）出来的，这意味着在反向传播计算梯度时，它被视为一个常数权重，因此不会对 $\log P_{\theta}$ 本身的梯度产生影响。

现在，我们来推导DFT目标的梯度。由于 $\pi_{\theta}(a_i | s_i)$ 项被分离，梯度计算如下：
$$\nabla_{\theta} L_{DFT}(\theta) = - \mathbb{E}_{(s_i, a_i) \sim D} [\pi_{\theta}(a_i | s_i) \cdot \nabla_{\theta} \log \pi_{\theta}(a_i | s_i)]$$
将这个梯度形式与策略梯度定理 $\mathbb{E}$ 进行比较，我们可以发现，这个新的缩放因子 $\pi_{\theta}(a_i | s_i)$ 充当了新的奖励信号。因此，DFT的隐式奖励 $R_{DFT}$ 为：
$$R_{DFT}(s_i, a_i) = \pi_{\theta}(a_i | s_i) \cdot R_{SFT}(s_i, a_i) = \pi_{\theta}(a_i | s_i) \cdot \frac{1}{\pi_{\theta}(a_i | s_i)} = 1$$
这一简单的乘法操作有效地“抵消”或“中和”了SFT中固有的病态反向概率加权问题。其结果是，对于每一个专家词元，无论模型当前的策略如何，其获得的奖励信号都是一个恒定的、行为良好的值“1”。这使得优化目标转变为以稳定且均匀加权的梯度来学习专家分布，从而极大地稳定了训练过程。

该方法的优雅之处在于其实现上的极致简洁，作者强调这仅仅是一行代码的改动：
`loss = loss * torch.softmax(shift_logits, dim=-1).gather(1, shift_labels.unsqueeze(-1)).squeeze(-1).detach()`

从更深层次看，SFT作为一种行为克隆方法，天然会受到协变量偏移（covariate shift）问题的影响。强化学习中通常使用重要性采样（importance sampling）来修正这种分布不匹配。SFT梯度中隐含的 $1/\pi_{\theta}$ 奖励项，形式上类似于离线策略修正中的一个部分，但其应用方式不当，反而导致了方差爆炸。DFT通过乘以 $\pi_{\theta}$ 来抵消这一项，可以被看作是对SFT内部一个有缺陷的、隐式的离线修正机制的纠正。它没有引入复杂的概率比率，而是将目标简化为一个稳定的、类似在线（on-policy）的更新过程，其中每个专家行为都被赋予了统一的正面奖励。

---

## 📊 Experiments

### 实验设计

- **模型**：实验采用了一系列当前先进的开源模型，涵盖了数学专用模型（如Qwen2.5-Math-1.5B/7B, DeepSeekMath-7B）和通用模型（如LLaMA-3.2-3B, LLaMA-3.1-8B），以验证方法的普适性。
- **数据集**：训练数据使用了NuminaMath CoT数据集的一个10万条样本的子集。评估则在一系列公认的、能测试模型深度推理和泛化能力的数学基准上进行，包括Math500、Minerva Math、Olympiad Bench、AIME 2024和AMC 2023。
- **基线方法**：DFT主要与标准SFT进行比较。同时，为了提供更广阔的视角，实验还包含了与同期工作Importance-Weighted SFT (iw-SFT)的对比，并在一个离线RL场景中，与DPO、RFT、PPO和GRPO等强基线进行了比较。

### 核心性能分析：DFT vs. SFT

实验结果清晰地表明，在所有测试模型和基准上，DFT的性能都显著优于标准SFT。尤其是在那些SFT表现不佳甚至产生负面影响（即性能下降）的高难度数据集上，DFT的优势更为突出，这有力地证明了其在改善泛化和避免过拟合方面的能力

| Model                   | Math500   | Minerva Math | Olympiad Bench | AIME24   | AMC23     | Avg.      |
| :---------------------- | :-------- | :----------- | :------------- | :------- | :-------- | :-------- |
| LLaMA-3.2-3B w/SFT      | 8.65      | 2.38         | 2.06           | 0.00     | 3.13      | 3.24      |
| LLaMA-3.2-3B w/DFT      | **12.79** | **2.84**     | **2.90**       | **0.83** | **3.91**  | **4.65**  |
| LLaMA-3.1-8B w/SFT      | 16.85     | 5.78         | 3.88           | 0.00     | 5.16      | 6.33      |
| LLaMA-3.1-8B w/DFT      | **27.44** | **8.26**     | **6.94**       | **0.41** | **12.03** | **11.02** |
| DeepSeekMath-7B w/SFT   | 26.83     | 7.26         | 6.33           | 0.41     | 8.28      | 9.82      |
| DeepSeekMath-7B w/DFT   | **41.46** | **16.79**    | **15.00**      | **1.24** | **16.25** | **18.15** |
| Qwen2.5-Math-1.5B w/SFT | 43.76     | 13.04        | 12.63          | 1.87     | 18.75     | 18.01     |
| Qwen2.5-Math-1.5B w/DFT | **64.89** | **20.94**    | **27.08**      | **6.87** | **38.13** | **31.58** |
| Qwen2.5-Math-7B w/SFT   | 53.96     | 16.66        | 18.93          | 2.48     | 26.09     | 23.62     |
| Qwen2.5-Math-7B w/DFT   | **68.20** | **30.16**    | **33.83**      | **8.56** | **45.00** | **37.15** |

### 在RL环境下的对比分析

在一个探索性的离线RL设定中，DFT再次取得了最佳的整体性能。以Qwen2.5-Math-1.5B模型为例，DFT的平均分达到了35.43，不仅超越了DPO（23.20）和RFT（23.97）等强大的离线RL方法，甚至优于PPO（28.66）和GRPO（32.00）等需要在线采样、更为复杂的在线RL算法。

| Setting                         | Math500   | Minerva Math | Olympiad Bench | AIME24   | AMC23     | Avg.      |
| :------------------------------ | :-------- | :----------- | :------------- | :------- | :-------- | :-------- |
| Qwen2.5-Math-1.5B w/SFT         | 43.14     | 11.64        | 13.41          | 1.03     | 14.84     | 16.81     |
| Qwen2.5-Math-1.5B w/DPO Offline | 46.89     | 11.53        | 22.86          | 4.58     | 30.16     | 23.20     |
| Qwen2.5-Math-1.5B w/RFT Offline | 48.23     | 14.19        | 22.29          | 4.37     | 30.78     | 23.97     |
| Qwen2.5-Math-1.5B w/PPO Online  | 56.10     | 15.41        | 26.33          | 7.50     | 37.97     | 28.66     |
| Qwen2.5-Math-1.5B w/GRPO Online | 62.86     | 18.93        | 28.62          | **8.34** | 41.25     | 32.00     |
| Qwen2.5-Math-1.5B w/DFT Offline | **64.71** | **25.16**    | **30.93**      | 7.93     | **48.44** | **35.43** |

### 模型行为与消融研究

- **“极化效应”**：对词元概率分布的分析显示，DFT会引导模型产生一种“极化效应”，即显著提升一部分关键语义词元的概率，同时主动抑制其他（如语法功能性）词元的概率，形成双峰分布。这与SFT倾向于统一提升所有词元概率的行为形成鲜明对比，表明DFT采用了一种更精细的学习策略，这可能也是其泛化能力更强的原因之一。
- **超参数鲁棒性**：消融实验表明，在不同的学习率和批量大小下，DFT的性能始终优于SFT。这证明了DFT的优势并非源于特定的超参数调优，而是其方法本身的根本性优势。

---

## 💬 Personal Insights

### SFT-RL谱系：DFT的定位

- **SFT (监督微调)**：可视为朴素的行为克隆。其目标纯粹是模仿，即最大化专家序列的概率。优点是简单高效，但其根本缺陷在于一个病态的隐式奖励，导致模型倾向于记忆而非泛化。
- **RLHF (人类反馈强化学习)**：这是一个复杂的多阶段优化过程。它需要训练一个独立的奖励模型，并使用PPO等在线RL算法进行策略优化。优点是能够探索并发现比SFT泛化能力更强的策略 ，但缺点是流程复杂、计算成本高且训练不稳定。
- **DFT (动态微调)**：DFT在这两个极端之间开辟了一个独特的中间地带。它保留了SFT的简洁和单阶段高效性，但通过数学上的修正，引入了RL的核心原则——一个定义良好的奖励信号。它在不引入显式奖励模型或在线采样等复杂组件的情况下，实现了类似RL的泛化收益，堪称一种“两全其美”的方法。

### 两种加权方案的辨析：DFT vs. Focal Loss

为了更深入地理解DFT中损失加权机制的精妙之处，可以将其与另一个著名的损失函数——焦点损失（Focal Loss）进行对比分析 。Focal Loss最初被提出用于解决密集目标检测任务中极端的前景-背景类别不平衡问题。

- **核心目标不同**：

  - **Focal Loss**：对于那些模型已经能够很好分类的样本（即“简单样本”，pt​很高），调制因子$(1-p_t)^\gamma$会变得非常小，从而极大地**降低这些简单样本对总损失的贡献**。这样一来，模型的训练将更加专注于那些难以分类的“困难样本”（pt​较低），即少数的正样本（目标物体）。
  - **DFT**：其目标是修正序列生成任务中一个**病态的隐式奖励结构**。它关心的不是词元间的类别不平衡，而是由反向概率奖励信号引起的训练不稳定问题。

- **数学形式与机制不同**：
  - **Focal Loss**: $FL(p_t) = -\alpha_t(1 - p_t)^\gamma \log(p_t)$ 。其目的是通过**抑制简单样本**的损失，来放大稀有困难样本的信号。
  - 其关键是调制因子 $(1 - p_t)^\gamma$。当样本被轻松分类时（$p_t \to 1$），该因子趋近于0，从而降低其损失贡献。当样本难以分类时（$p_t \to 0$），该因子接近1，损失不受影响。它根据**分类的难易程度**进行加权。
  - **DFT**: $L_{DFT} = -p_t \cdot \log(p_t)$（简化形式）。其目的是通过**抑制困难样本**（低pt​）的损失，来稳定整体的梯度更新。
    - 其关键是缩放因子 $p_t$。这个因子的唯一目的是在代数上**抵消**SFT梯度中隐式的 $1/p_t$ 奖励项。它并非为了区分难易样本，而是为了将所有专家样本的奖励**均等化**为一个恒定值1。它为了实现**奖励的稳定性**而进行加权。

这一对比揭示了一个深刻的现象：两种成功的损失加权方案，采用了几乎相反的策略。Focal Loss提升了困难样本的相对权重，而DFT降低了困难样本的相对权重。这种看似矛盾的现象背后，反映了对不同问题领域核心瓶颈的精准把握。

- 在密集目标检测中，学习的**瓶颈是信噪比**。绝大多数的“简单背景”样本构成了压倒性的噪声，淹没了少数“困难前景”样本的信号。因此，Focal Loss通过抑制噪声（简单样本）来放大信号（困难样本），解决了信号检测的问题。
- 在大型语言模型的自回归生成任务中，学习的**瓶颈是优化稳定性**。那些模型难以预测的“困难词元”（即使它们是正确答案）会产生巨大的梯度，形成破坏性的“梯度噪声”，导致模型无法稳定收敛到泛化能力强的解。因此，DFT通过抑制这种梯度噪声（困难样本），来保证学习过程的平稳性，解决了优化路径的问题。

这个对比分析表明，“根据概率对损失进行加权”这一思想本身并非万能药，其具体的函数形式和作用机制必须与待解决问题的核心矛盾相匹配。DFT和Focal Loss的成功，恰恰在于它们都识别并精确地调制了各自领域中信息量最低、但对训练过程干扰最大的那部分数据分布的贡献。DFT通过优先学习模型已经能够较好理解的部分，鼓励其构建一个连贯、自洽的内部知识体系，而不是强迫它去死记硬背那些与之当前理解相悖的“例外情况”。

### SFT与DFT损失函数的信息论诠释

#### SFT损失：最小化KL散度

从信息论的角度来看，SFT所使用的标准交叉熵损失函数，其根本目标是最小化两个概率分布之间的差异。
在机器学习的分类任务中，我们有两个关键的概率分布：

1. **真实数据分布 P**：在SFT中，这通常由训练数据集的经验分布来表示。对于每个词元的预测，真实分布是一个one-hot向量，即正确词元的概率为1，其余所有词元的概率为0。
2. **模型预测分布 Q**：这是模型（由参数$\theta$决定）在给定上下文后，对下一个词元的预测概率分布。

交叉熵（Cross-Entropy）$H(P, Q)$衡量了使用基于分布$Q$的编码方式来编码来自真实分布P的样本时，所需要的平均比特数 。在SFT的场景下，由于P是one-hot分布（假设正确类别是j），交叉熵就简化为了我们熟悉的负对数似然损失。
信息论中一个更为核心的概念是KL散度（Kullback-Leibler Divergence），也称为相对熵，它直接度量了从一个概率分布P到另一个概率分布Q的差异 。KL散度与交叉熵以及真实数据分布P的香农熵（Shannon Entropy）之间存在一个基本关系 ：
$H(P,Q)=H(P)+D_{KL}​(P∣∣Q)$

香农熵衡量了真实数据分布P自身的不确定性。由于在SFT的训练过程中，训练数据集是固定的，因此真实数据分布P及其熵$H(P)$是一个常数。这意味着，最小化交叉熵损失$H(P, Q)$在数学上完全等价于最小化KL散度$D_{KL}(P || Q)$。因此，SFT的全部优化过程可以被精确地理解为：调整模型参数$\theta$以改变其预测分布$Q$，使其与真实数据分布P的KL散度尽可能小，最终目标是让Q成为P的完美复刻。

#### 修正DFT损失：从熵最小化到稳定化信息获取

DFT对单个词元的损失项形式为$-p \log p$，这与香农熵公式$H(Q)=−∑_i​p_i​logp_i​$中的单个项完全一致。然而，我们需要对这一观察的**目标**和**机制**进行更精确的界定。

首先，DFT的优化目标**不是**最小化模型预测分布Q的总熵H(Q)。如果直接以最小化$H(Q)$为目标，模型会被激励在任何情况下都做出极度自信的预测（即输出一个熵极低的、高度集中的概率分布），而完全忽略了地面真实标签。这显然会导致模型与现实脱节。

DFT的真正作用，是利用这个熵形式的项$-p_t \log p_t$（其中pt​是正确词元的概率）作为对标准交叉熵损失$-\log p_t$的**加权因子**。这种加权机制的核心功能是**稳定学习信号**。从信息论的角度，我们可以这样理解：KL散度$D_{KL}(P || Q)$可以被解释为“意外程度”（Surprisal）的期望值 。SFT通过最小化KL散度，表现得像一个“焦虑的”学习者，它对任何与自己预期不符的、感到“意外”的数据点都反应过度。其`1/p`的隐性奖励结构正是这种焦虑的体现：越是意外，反应越是剧烈。

相比之下，DFT通过乘以概率pt​来对损失进行加权，表现得像一个“自信的”学习者。它承认数据中存在意外之处，但它不允许这些意外来主导和颠覆整个学习过程。它更相信自己已有的世界模型，并在此基础上进行平滑、渐进的更新，假设新的数据最终能够被整合进一个连贯的知识体系中。

我们可以将梯度的幅度视为模型从一个数据点吸收信息的速率。SFT的信息吸收速率是极不稳定的、尖峰状的，它会从“意外”的词元中吸收巨量信息，而从“符合预期”的词元中吸收少量信息。DFT则平滑了这个过程。通过将损失乘以pt​，它确保了模型从任何一个正确的词元中吸收的信息量，与模型对该词元已有的“信念”强度成正比。这避免了模型被“强行灌输”它尚未准备好处理的信息，从而实现了更好的信息“消化”和知识内化。

#### 微调抉择

- SFT
  - **基础模型能力较弱或缺乏领域知识时**：当基础模型对目标领域几乎没有先验知识时，DFT的“引导”能力无从发挥。此时，SFT的直接模仿虽然泛化性差，但可能是让模型快速学习基本格式和术语的唯一途径。
  - **任务简单，侧重于风格模仿或格式遵循时**：如果任务的核心是学习一种特定的输出风格、JSON格式或简单的文本转换，而非复杂的逻辑推理，SFT的记忆能力通常已经足够，且实现最为简单。
  - **当计算和实现的简洁性是首要考量时**：尽管DFT的改动极小，但在某些极端追求简洁的流程中，标准SFT仍是最基础的选择。
- DFT
  - **基础模型强大且具备相关领域潜在能力时**：这是应用DFT的最理想场景。选择一个在目标领域（如数学、代码）经过预训练或已知表现出色的基础模型，然后使用DFT来“解锁”和“引导”其潜力，能够获得最佳效果 。
  - **任务复杂，要求多步推理和强泛化能力时**：对于数学题解答、复杂指令遵循、代码生成等任务，泛化能力至关重要。实验明确表明，在这些任务上DFT远胜于SFT。
  - **寻求以SFT成本实现类RL泛化效果时**：DFT提供了一条极具吸引力的中间路线。它避免了传统RLHF（如PPO）或DPO所需的复杂流程（如奖励模型训练、在线采样或偏好数据收集），却能在很大程度上弥补SFT与RL之间的泛化差距，极大地提高了对齐训练的效率和可及性。

---

## 🔗 推导

### 问题设定与符号

- 训练数据集：$\mathcal{D} = \{(x, y^\star)\}$，其中 $x$ 是输入（prompt），
  $y^\star = (y^\star_1, \dots, y^\star_T)$ 是专家给出的完整序列（固定目标序列）。
- 参数化策略（语言模型）：$\pi_\theta(y\mid x)$。链式分解为：
  $$
  \pi_\theta(y\mid x) = \prod_{t=1}^T \pi_\theta(y_t \mid y_{<t}, x).
  $$
- 记号：$\log\pi_\theta(y\mid x)=\sum_{t=1}^T \log\pi_\theta(y_t\mid y_{<t},x)$。

### SFT目标与直接梯度

SFT 的目标（逐序列交叉熵）定义为：

$$
L_{\mathrm{SFT}}(\theta) = \mathbb{E}_{(x,y^\star)\sim\mathcal{D}}\big[-\log\pi_\theta(y^\star\mid x)\big].
$$

对参数 $\theta$ 的梯度（显式写出期望）为：

$$
\nabla_\theta L_{\mathrm{SFT}}(\theta) = -\,\mathbb{E}_{(x,y^\star)\sim\mathcal{D}}\big[\nabla_\theta\log\pi_\theta(y^\star\mid x)\big].
$$

这是最常见的交叉熵训练（teacher-forcing）。

### 把 SFT 改写为 policy-gradient 的形式

#### 重要抽样（Importance Sampling）

##### 直观理解

当我们想估计某个分布 $p$ 下的期望 $\mathbb{E}_{y\sim p}[f(y)]$，但直接从 $p$ 采样困难或不方便时，可以改用另一个更容易采样的分布 $q$。从 $q$ 采样后，用权重把样本“改回去”，这就是**重要抽样**的思想：用 $q$-样本加权得到 $p$ 下的期望估计。

直观类比：你想估计野外鱼群的平均重量（真正的分布 $p$），但你更容易在养殖场采样（分布 $q$），于是用养殖场样本并按比例权重来恢复对野生分布的估计。

目标：证明对任意可积函数 $f$，

$$
\mathbb{E}_{y\sim p}[f(y)] \;=\; \mathbb{E}_{y\sim q}\!\left[\frac{p(y)}{q(y)}\, f(y)\right],
$$

前提是：对所有 $y$ 若 $p(y)>0$ 则 $q(y)>0$（避免除零），并且比值可积。

**离散情形证明（直观）**：

$$
\begin{align*}
\mathbb{E}_{y\sim p}[f(y)] &= \sum_y p(y) f(y) \\
&= \sum_y q(y)\,\frac{p(y)}{q(y)}\,f(y) \\
&= \mathbb{E}_{y\sim q}\!\left[\frac{p(y)}{q(y)} f(y)\right].
\end{align*}
$$

连续情形同理，用积分替换求和。由此得到**无偏估计量**：若从 $q$ 抽取 $N$ 个独立样本 $y^{(i)}$，则

$$
\hat\mu = \frac{1}{N}\sum_{i=1}^N \frac{p(y^{(i)})}{q(y^{(i)})}\, f(y^{(i)})
$$

满足 $\mathbb{E}[\hat\mu] = \mathbb{E}_{p}[f]$。

假设样本空间 $\{a,b\}$：

- 真实分布 $p$: $p(a)=0.8,\ p(b)=0.2$；
- 重要分布 $q$: $q(a)=0.5,\ q(b)=0.5$；
- 函數值 $f(a)=2,\ f(b)=4$。

直接计算：

$$
\mathbb{E}_p[f] = 0.8\cdot 2 + 0.2\cdot 4 = 1.6 + 0.8 = 2.4.
$$

用 importance sampling（从 $q$ 抽样）：

$$
\begin{align*}
\mathbb{E}_q\!\left[\frac{p}{q} f\right]
&= q(a)\frac{p(a)}{q(a)} f(a) + q(b)\frac{p(b)}{q(b)} f(b) \\
&= 0.5\cdot\frac{0.8}{0.5}\cdot 2 + 0.5\cdot\frac{0.2}{0.5}\cdot 4 = 1.6 + 0.8 = 2.4.
\end{align*}
$$

#### SFT → policy-gradient 推导中的角色

目标：将上式表示成在模型采样分布 $y\sim\pi_\theta(\cdot\mid x)$ 下的期望，从而显式出现重要性权重并与 policy-gradient 对比。

对任意可测函数 $f(x,y)$，重要抽样恒等式为：

$$
\mathbb{E}_{(x,y)\sim\mathcal{D}}[f(x,y)]
= \mathbb{E}_{x\sim D_x}\Big[\sum_y p_{\mathcal{D}}(y\mid x)\,f(x,y)\Big]
= \mathbb{E}_{x\sim D_x,\ y\sim\pi_\theta(\cdot\mid x)}\Big[\frac{p_{\mathcal{D}}(y\mid x)}{\pi_\theta(y\mid x)}\,f(x,y)\Big].
$$

在训练数据中通常 $p_{\mathcal{D}}(y\mid x)=\mathbf{1}[y=y^\star]$（指示函数）。设

$$
w(y\mid x)=\frac{1}{\pi_\theta(y\mid x)},\qquad r(x,y)=\mathbf{1}[y=y^\star].
$$

把 $f(x,y)=-\nabla_\theta\log\pi_\theta(y\mid x)$ 代入，得到：

$$
\nabla_\theta L_{\mathrm{SFT}}(\theta)
= -\,\mathbb{E}_{x\sim D_x,\ y\sim\pi_\theta}\Big[\frac{\mathbf{1}[y=y^\star]}{\pi_\theta(y\mid x)}\ \nabla_\theta\log\pi_\theta(y\mid x)\Big].
$$

紧凑写法为：

$$
\boxed{\displaystyle \nabla_\theta L_{\mathrm{SFT}}(\theta)
= -\,\mathbb{E}_{x,y\sim\pi_\theta}\big[ w(y\mid x)\,r(x,y)\,\nabla_\theta\log\pi_\theta(y\mid x)\big],}
$$

其中 $w=1/\pi$。这就是把 SFT 看成 policy-gradient 的关键恒等式，但注意出现了重要性权重 $1/\pi$（可能导致数值问题）。

### 为什么 $1/\pi$ 会导致方差 / 数值问题

#### 定义：二阶矩

对于$X$，其 二阶矩 通常指 $\mathbb{E}[X^2]$（标量情形）或 $\mathbb{E}[\|X\|^2]$（向量情形，$\|\cdot\|$ 为欧几里得范数）。二阶矩不是方差：

$$
\mathrm{Var}(X) \;=\; \mathbb{E}\big[(X-\mathbb{E}[X])^2\big] \;=\; \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
$$

因此若 $\mathbb{E}[X^2]$ 很大，即使 $\mathbb{E}[X]$ 不算大，方差也可能很大。

使用重要抽样时，单样本的贡献为$W(y) f(y)$，其中 $W=p/q$，单样本贡献二阶矩 $\mathbb{E}_q[(W f)^2]$ 。若 $W$ 有重尾（某些样本的 $W$ 很大），则 $\mathbb{E}_q[W^2 f^2]$ 会非常大，导致方差巨大、不穩定。

直观：被 $q$ 覆盖但概率绩效的样本，会因 $W$ 很大而在被放大，单个罕见样本可能主导整个计算。

对于固定 $x$，单样本（SFT 在 policy-gradient 视角下）是：

$$
G(x,y) \;=\; -\,\frac{\mathbf{1}[y=y^\star]}{\pi_\theta(y\mid x)}\,\nabla_\theta\log\pi_\theta(y\mid x).
$$

计算二阶矩（对 $y\sim\pi_\theta$）：

$$
\begin{align*}
\mathbb{E}_{y\sim\pi}\big[\|G\|^2\big]
&= \pi_\theta(y^\star\mid x)\ \Big\|\frac{1}{\pi_\theta(y^\star\mid x)}\,\nabla_\theta\log\pi_\theta(y^\star\mid x)\Big\|^2 \\[4pt]
&= \frac{1}{\pi_\theta(y^\star\mid x)}\ \big\|\nabla_\theta\log\pi_\theta(y^\star\mid x)\big\|^2.
\end{align*}
$$

因此当 $\pi_\theta(y^\star\mid x)$ 很小时（例如 $0.01$），二阶矩会被放大（按 $1/\pi$ 比例），导致方差巨大。

### DFT 的核心想法与数学表达（stop-gradient 的作用）

**数值上的修改**：把每个专家 token 的交叉熵项乘以对应的模型概率（数值上），但对该概率使用 stop-gradient（不让该概率参与反向传播）。

序列级直观写法（带 stop-gradient）为：

$$
\nabla_\theta L_{\mathrm{DFT}}(\theta)
= -\,\mathbb{E}_{(x,y^\star)\sim\mathcal{D}}\big[\mathrm{sg}\big(\pi_\theta(y^\star\mid x)\big)\,\nabla_\theta\log\pi_\theta(y^\star\mid x)\big],
$$

其中 $\mathrm{sg}(\cdot)$ 表示 stop-gradient（前向计算使用数值，反向不传播）。

用重要抽样改写为在 $y\sim\pi_\theta$ 下的期望：

$$
\nabla_\theta L_{\mathrm{DFT}}(\theta)
= -\,\mathbb{E}_{x,y\sim\pi_\theta}\Big[\frac{\mathbf{1}[y=y^\star]}{\pi_\theta(y\mid x)}\ \mathrm{sg}\big(\pi_\theta(y^\star\mid x)\big)\ \nabla_\theta\log\pi_\theta(y\mid x)\Big].
$$

在 $y=y^\star$ 的情形，数值上有：

$$
\frac{\mathrm{sg}\big(\pi_\theta(y^\star\mid x)\big)}{\pi_\theta(y^\star\mid x)}
= \frac{\pi_\theta(y^\star\mid x)}{\pi_\theta(y^\star\mid x)} = 1.
$$

（这里的关键是：`sg` 不改变前向数值，只阻断梯度。）因此数值上整项变为：

$$
-\,\mathbb{E}_{x,y\sim\pi_\theta}\big[\mathbf{1}[y=y^\star]\,\nabla_\theta\log\pi_\theta(y\mid x)\big],
$$

也就是

$$
\boxed{\nabla_\theta L_{\mathrm{DFT}}(\theta)
= -\,\mathbb{E}_{x\sim D_x}\big[\pi_\theta(y^\star\mid x)\,\nabla_\theta\log\pi_\theta(y^\star\mid x)\big].}
$$

$$
\mathbb{E}[\|G_{\text{DFT}}\|^2] = \pi_\theta(y^\star\mid x)\ \|\nabla_\theta\log\pi_\theta(y^\star\mid x)\|^2,
$$

这个表达在数值上不再包含 $1/\pi$，从而显著降低二阶矩方差；在 RL 术语下相当于把 reward 矫正为恒等于 $1$ 的（更稳定的）策略梯度估计。

### Token-level 等价实现

令 logits 为 $z_k$，softmax 定义为 $\pi(k) = \frac{e^{z_k}}{\sum_j e^{z_j}}$，则

$$
\frac{\partial \log\pi(i)}{\partial z_k} = \mathbf{1}[k=i] - \pi(k).
$$

因此对参数 $\theta$ 有

$$
\nabla_\theta \log\pi(i) = \sum_k (\mathbf{1}[k=i]-\pi(k))\,\nabla_\theta z_k
= \nabla_\theta z_i - \sum_k \pi(k)\,\nabla_\theta z_k.
$$

在 DFT 的 token-level 梯度中：

$$
\nabla_\theta\big(-\mathrm{sg}(\pi)\log\pi\big) = -\mathrm{sg}(\pi)\,\nabla_\theta\log\pi,
$$

因为 $\mathrm{sg}(\pi)$ 在反向传播时被视为常数（不依赖于 $\theta$）。

为了数值稳定与实现方便，通常将 sequence-level 的乘子拆成逐 token 的乘子。在训练时使用下面的 loss：

$$
L_{\mathrm{DFT}}(\theta)
= \mathbb{E}_{(x,y^\star)\sim\mathcal{D}}\Big[ -\sum_{t=1}^T \mathrm{sg}\big(\pi_\theta(y^\star_t\mid y^\star_{<t},x)\big)\ \log\pi_\theta(y^\star_t\mid y^\star_{<t},x)\Big].
$$

逐 token 展开后，每个 token 的 $1/\pi$ 都会被其对应的 $\pi$（数值）抵消，从而使得 token 级别的估计量没有 $1/\pi$ 的放大因子。

**实现细节（PyTorch 风格伪代码）**：

```python
import torch
import torch.nn.functional as F

# logits: (batch, seq_len, vocab)
# target: (batch, seq_len)  (目标序列，pad 已处理)
# loss_mask: (batch, seq_len)  (1: 有效 token, 0: padding)

logits = model(input_ids, attention_mask=...)  # (B, T, V)
log_probs = F.log_softmax(logits, dim=-1)     # (B, T, V)

# target_logp: (B, T)
target_logp = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)

# token-level model-probabilities (数值乘子)，并 stop-gradient
token_prob = target_logp.exp().detach()      # (B, T)

# dft per-token loss, 注意 detach 只对 token_prob 生效
per_token_loss = - token_prob * target_logp   # (B, T)

# mask padding tokens
per_token_loss = per_token_loss * loss_mask

# final loss: mean over non-pad tokens
loss = per_token_loss.sum() / loss_mask.sum()

loss.backward()
# optimizer.step(), etc.
```

关键点：使用 `token_prob = target_logp.exp().detach()` 确保 `token_prob` 在反向传播时没有梯度，这样梯度只来自 `target_logp`（即 $\log\pi$），乘子只是数值缩放因子。
