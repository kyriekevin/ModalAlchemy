---
title: Inference-Time Scaling for Generalist Reward Modeling
date: 2025-08-19T00:00:00.000Z
tags:
  - rlhf
  - grpo
  - rft
  - reward_model
category: paper
venue: arXiv
authors: zyz
---
# GRM

## 📚 Table of Contents

- [📖 Paper Info](#-paper-info)
- [📝 Summary](#-summary)
- [🔑 Key Contributions](#-key-contributions)
- [🧩 Method](#-method)
- [📊 Experiments](#-experiments)
- [💬 Personal Insights](#-personal-insights)
- [🔗 Related Papers](#-related-papers)

---

## 📖 Paper Info

- **Title:** Inference-Time Scaling for Generalist Reward Modeling
- **Authors:** DeepSeek
- **Venue:** arXiv
- **Paper Link:** [https://arxiv.org/abs/2504.02495](https://arxiv.org/abs/2504.02495)
- **Publication Date:** 2025-04-05

---

## 📝 Summary

解决当前LLM Reward Models在**泛化能力**和**推理时可扩展性**方面面临的关键挑战。传统奖励模型通常输出一个单一的标量分数来表示偏好，这种方式不仅限制了其在不同类型输入上的灵活性，也使其难以通过增加推理时的计算量来提升性能。

为了克服这些局限，论文提出了一套创新的方法论，其核心包含两个层面：

1. **范式转变：** 采用**逐点生成式奖励模型（Pointwise Generative Reward Modeling, GRM）**。与传统RM直接输出分数不同，GRM通过生成一段自然语言文本（包含评价原则和具体批判）来评估回复的质量，最终从该文本中提取出奖励分数。这种生成式的方法天然具备处理多样化输入（单个、成对、多个回复）的灵活性，并为推理时性能扩展奠定了基础。
2. **训练方法创新：** 提出**自洽原则批判调优（Self-Principled Critique Tuning, SPCT）**。这是一种新颖的、分为两个阶段的训练流程，旨在培养GRM生成高质量、可扩展奖励信号的能力。它首先通过“拒绝式微调”进行冷启动，教会模型生成符合格式的原则与批判；然后通过“基于规则的在线强化学习”进行优化，激励模型生成能更准确区分优劣回复的批判内容。

通过SPCT训练的GRM，可以有效地利用**推理时扩展（Inference-Time Scaling）** 技术（如并行采样和投票机制）来显著提升性能。实验证明，一个27B参数的DeepSeek-GRM模型，在经过推理时扩展后，其在多个通用奖励模型基准测试上的表现，能够媲美甚至超越一个参数量高达671B的MoE（Mixture-of-Experts）模型 。

---

## 🔑 Key Contributions

该研究的核心贡献可以从三个层面进行解构，它们共同构建了一个从训练方法到推理应用的完整体系。

1. **提出SPCT：一种旨在培养奖励模型可扩展行为的新型学习方法**
   传统方法训练模型去“预测”一个偏好分数，而SPCT训练模型去完成一项“推理”任务：生成评价原则（principles）和具体批判（critiques）。这种方法培养了模型“可扩展的奖励生成行为”，即随着模型在推理时被赋予更多的计算预算（例如，生成更多样化的原则和批判），其最终输出的奖励信号质量也随之提高。其内在的逻辑是，SPCT将奖励建模从一个简单的回归问题，提升到了一个模拟人类基于原则进行判断的认知过程。模型不再是学习一个不透明的分数函数，而是学习一个可解释的、结构化的、能够产出分数的推理流程。这个流程本身是生成性的，因此可以通过多次采样来探索不同的推理路径。

2. **引入Meta RM引导的投票机制，实现更高效的推理时扩展**
   论文并未止步于简单的多数投票或均值投票。为了解决并行采样中可能产生低质量或无关批判的问题，研究者引入了一个**元奖励模型（Meta Reward Model, Meta RM）**。这个Meta RM是一个次级评判者，其专门任务是评估主GRM生成的每一份批判文本的质量和“正确性”。在推理时，系统首先由主GRM生成多个候选批判，然后由Meta RM对这些批判进行打分，最后只选择那些由Meta RM评出的高质量批判来进行最终的投票决策。这种机制建立了一种分层的推理结构，相当于为奖励模型本身配备了一个“内部批评家”。它将简单的“量”的累加（投票）转变为“质”的筛选。

3. **提供强有力的实证：证明推理时扩展可与训练时扩展相媲美**
   通过大量的实验，作者清晰地展示了其27B参数的DeepSeek-GRM模型，在利用32个样本进行推理时扩展后，其综合性能表现与参数规模为其25倍的DeepSeek-V3（671B MoE）模型相当。

---

## 🧩 Method

该论文的方法论构建了一个环环相扣的系统，从奖励模型的根本范式选择，到精巧的训练策略，再到最终的推理时扩展机制，共同实现了一个可扩展的通用奖励系统。

### The Landscape of Reward Modeling: A Comparative Analysis

在深入探讨GRM之前，理解现有奖励模型（RM）的局限性至关重要。一个理想的通用奖励模型需要具备两大特性：**输入灵活性**（能处理单个、成对或多个回复）和**推理时可扩展性**（能通过增加计算来提升性能）。论文通过对现有RM范式的分析，指出了它们在这些方面的不足，从而论证了选择GRM路线的必要性 。

- **标量RM (Scalar RM)**：这是最传统的范式，直接输出一个标量分数。它的主要问题在于输入灵活性差（通常被设计为成对比较），且推理时扩展潜力低。因为其输出是确定性的，多次采样几乎不会产生方差，无法通过集成来提升性能。
- **半标量RM (Semi-Scalar RM)**：这类模型在输出标量分数的同时，会附带一些元数据。它在灵活性上略有提升，但其核心输出仍是标量，因此在推理时扩展的能力同样受限。
- **生成式RM (GRM)**：与前两者根本不同，GRM将奖励建模视为一个语言生成任务。它不直接输出分数，而是生成一段包含原则和批判的文本，分数是从文本中解析出来的。这种范式天然地解决了输入灵活性的问题，更重要的是，其生成过程为推理时扩展打开了大门。通过采样，模型可以生成多样化的推理路径（即不同的原则和批判），并通过投票等方式聚合这些路径，从而有效提升判断的准确性和鲁棒性。

下表清晰地对比了不同奖励模型范式在关键特性上的差异，凸显了GRM的优势，这也是论文选择此技术路线的根本原因 。

| 范式                          | 模型输出                              | 输入灵活性                    | 推理时扩展潜力                |
| :---------------------------- | :------------------------------------ | :---------------------------- | :---------------------------- |
| **标量RM (Scalar RM)**        | 单一标量值 (e.g., $s \in \mathbb{R}$) | 有限 (通常为成对比较)         | 较低 (输出确定性高，方差小)   |
| **半标量RM (Semi-Scalar RM)** | 标量值 + 少量元数据                   | 中等                          | 有限                          |
| **生成式RM (GRM)**            | 自然语言文本 (批判+原则)              | 高 (支持单个、成对、多个回复) | 高 (可通过采样生成多样化输出) |

### The Foundation: Pointwise Generative Reward Modeling (GRM)

GRM是整个方法论的基石。它将奖励建模重新定义为一个生成任务，而非回归或分类任务。具体而言，当给定一个查询（query）和一个或多个候选回复（responses）时，GRM并不直接输出数值分数，而是生成一段结构化的自然语言文本，这段文本包含了评价的**原则（Principles）** 和针对每个回复的**批判（Critiques）**。最终，一个确定的数值分数（例如1到10分）会从这段生成的文本中被解析出来。

GRM范式相比传统方法，具备三大核心优势：

- **输入灵活性（Input Flexibility）**：传统成对比较的RM（Pairwise RM）难以处理单个回复或多个回复（Best-of-N）的评分。GRM通过生成文本的方式，天然地统一了对任意数量回复的评价，极大地增强了在实际应用中的灵活性和便利性。
- **可解释性（Interpretability）**：生成的批判文本为我们提供了一个观察RM“思考过程”的窗口。当RM给出一个判断时，我们可以直接阅读其背后的理由，这使得模型的决策过程不再是一个黑箱，极大地便利了模型的调试、分析和对齐验证。
- **内在可扩展性（Inherent Scalability）**：作为一个语言生成任务，GRM天然适用于各种基于采样的推理时技术。通过调整采样温度（temperature），模型可以生成多样化的批判文本，这正是论文后续提出的推理时扩展机制的根本前提。

### The Training Engine: Self-Principled Critique Tuning (SPCT)

SPCT是驱动GRM学习到强大泛化和扩展能力的核心训练引擎。它是一个精心设计的两阶段流程，旨在将一个通用的LLM转化为一个专业的、能生成高质量批判的奖励模型。

#### Phase 1: Cold Start with Rejective Fine-Tuning (RFT)

此阶段的目标是为模型进行“冷启动”，教会模型生成评价文本的基本**格式**，并使其初步与人类的偏好对齐。

- **目标**：学习生成包含“原则”和“批判”的结构化文本，并使提取出的分数与基准数据中的人类偏好（ground truth）初步一致。
- **数据**：混合使用了通用指令数据和通过“拒绝式采样”专门构建的数据 。
- **过程**：在拒绝式采样流程中，模型会针对已知偏好的人类数据生成批判轨迹。如果从生成轨迹中提取出的分数排序与人类偏好相悖，那么这条轨迹就会被**拒绝**，不用于模型训练。这种机制是一种有效的自监督和数据过滤，能剔除那些导致错误判断的推理路径。此外，该阶段还使用了“提示采样”（hinted sampling），即在输入中选择性地加入正确答案的提示，以帮助模型更快地学习到正确的判断模式。

#### Phase 2: Online Refinement with Rule-Based RL (GRPO)

如果说RFT阶段是“知识灌输”，那么在线强化学习阶段就是“实战演练”。此阶段的目标是超越简单的格式遵循和偏好复现，训练GRM生成**更有效（effective）** 的批判——即那些能够最精准地区分优劣回复的批判。

- **目标**：优化GRM的生成策略，使其生成的批判能够最大化最终排序的准确率。
- **算法**：论文采用了**生成式奖励策略优化（Generative Reward Policy Optimization, GRPO）** 算法 。在此框架中，GRM本身被视为一个策略网络（policy network）。
- **奖励函数**：RL的奖励信号是基于规则的。当GRM生成的批判所导出的回复排序与人类偏好完全一致时，模型会获得正向奖励。为了保证训练的稳定性和避免模型偏离已学到的良好行为（即所谓的“灾难性遗忘”），论文采用了较大的KL散度惩罚系数（$\beta$），以约束策略网络不会与RFT阶段训练好的模型偏离太远。

这种两阶段的训练结构，精妙地模拟了一个认知学习过程。RFT如同“理论学习”，让模型掌握了批判的“语法”和“词汇”。而在线RL阶段则如同“应用实践”，模型通过反复试错，学会了哪些“论证”（即批判）在实际判断中是最有说服力和最有效的。监督学习与强化学习的结合，是培养出鲁棒推理能力的关键。

### The Scaling Mechanism: From Single Judgment to Collective Consensus

训练好的SPCT模型为推理时扩展提供了可能。论文探索并验证了两种核心的扩展技术。

#### Core Technique: Parallel Sampling and Voting

这是最直接的扩展方式。在推理时，不再使用贪心解码（greedy decoding）生成唯一的批判文本，而是通过带温度的采样（temperature sampling, e.g., $T=0.5$）并行生成 $k$ 个不同的批判版本。

- **过程**：
  1. 对于一组待评估的回复，并行生成 $k$ 份独立的批判文本。
  2. 从每份批判文本中解析出对每个回复的分数。
  3. 将这 $k$ 份分数进行聚合（例如，对每个回复的分数求和），得到最终的、更稳健的评分 $S_i^* = \sum_{j=1}^{k} S_{i,j}$ 。
- **机制**：这种方法利用了模型在学习过程中形成的多样化推理路径。通过聚合多个独立“判断”的结果，可以有效降低单个判断的方差，提高整体的准确性和稳定性，其原理类似于机器学习中的集成方法（ensemble methods）。为了避免位置偏见，每次采样前会对输入的回复顺序进行随机打乱。

#### Advanced Technique: Meta RM-Guided Voting

简单的投票机制假设所有采样出的批判都具有同等的价值，但这在现实中并非如此。为了解决这个问题，论文引入了更高级的Meta RM引导投票机制。

- **问题**：在 $k$ 个采样结果中，可能包含一些逻辑混乱、原则不当或与问题无关的低质量批判。简单的求和投票可能会被这些“噪声”干扰。
- **解决方案**：训练一个独立的、更轻量的标量RM，即**Meta RM**。这个模型的唯一任务是判断主GRM生成的某一段批判文本本身是否“优质”或“正确” 。
- **过程**：
  1. 主GRM并行生成 $k$ 份批判文本。
  2. Meta RM对这 $k$ 份批判文本逐一进行评分。
  3. 系统只选用Meta RM评分最高的 $k_{meta}$（其中 $k_{meta} \le k$）份批判文本，并基于这些高质量的批判进行最终的投票。

这种机制通过引入一个“质量过滤器”，显著提升了投票的效率和最终结果的准确性，使得扩展过程更加智能和可控。

---

## 📊 Experiments

论文通过一系列全面的实验，对其提出的方法进行了严格的验证。实验设计覆盖了多个维度，包括与基线模型的性能对比、推理时扩展能力分析以及关键组件的消融研究。

### Benchmarks and Evaluation Framework

为了证明模型的“通用性”（generalist），实验采用了覆盖不同领域和任务的多个权威奖励模型基准测试集 ：

- **RewardBench**: 一个综合性基准，包含通用对话、推理和安全等多个维度的偏好数据。
- **PPE (Public Preference Elicitation)**: 专注于可验证任务（如数学、代码）的偏好数据，由众包产生。
- **RMB (Reward Model Bench)**: 一个全面的基准，侧重于评估模型的有益性（helpfulness）和无害性（harmlessness），包含成对比较和Best-of-N两种格式。
- **ReaLMistake**: 一个专门用于诊断单个回复中错误的基准测试，考察模型识别错误的能力。

评估指标采用了对应任务的标准：对于偏好排序任务，使用**准确率（Accuracy）**；对于错误诊断任务，使用**ROC-AUC** 。

### Key Experimental Results

实验结果清晰地表明了DeepSeek-GRM模型及其推理时扩展的优越性。下表汇总了关键模型的性能对比，展示了SPCT方法在同等模型规模下的领先地位，以及推理时扩展带来的巨大性能飞跃。

| 模型 (Model)                 | 参数规模 (Parameters) | 推理方式 (Inference) | RewardBench (Acc.) | PPE (Acc.) | RMB (Acc.) | 平均准确率 (Avg. Acc.) |
| :--------------------------- | :-------------------- | :------------------- | :----------------- | :--------- | :--------- | :--------------------- |
| LLM-as-a-Judge (Gemma-2-27B) | 27B                   | Greedy               | 79.8               | 80.5       | 74.3       | 78.2                   |
| DeepSeek-PairRM-27B          | 27B                   | -                    | 81.2               | 84.1       | 75.1       | 80.1                   |
| **DeepSeek-GRM-27B (Ours)**  | **27B**               | **Greedy**           | **83.5**           | **85.2**   | **77.6**   | **82.1**               |
| Nemotron-4-340B-Reward       | 340B                  | -                    | 84.1               | 87.9       | 78.5       | 83.5                   |
| GPT-4o (0513)                | Proprietary           | -                    | 85.3               | 88.5       | 79.2       | 84.3                   |
| **DeepSeek-GRM-27B (Ours)**  | **27B**               | **Scaling (k=32)**   | **85.8**           | **88.1**   | **80.3**   | **84.7**               |
| DeepSeek-GRM-671B MoE        | 671B MoE              | Greedy               | 86.0               | 88.3       | 79.9       | 84.7                   |

从上表中可以得出几个关键结论：

1. **同规模下的优越性**: 在27B参数规模下，使用SPCT训练的DeepSeek-GRM-27B在所有基准上均显著优于其他基线方法，如LLM-as-a-Judge和传统的成对奖励模型（PairRM），证明了SPCT训练方法的有效性。
2. **推理时扩展的巨大威力**: 通过使用32个样本进行推理时扩展，DeepSeek-GRM-27B的性能得到了大幅提升，平均准确率从82.1%跃升至84.7%。
3. **与巨型模型相媲美**: 最令人瞩目的是，经过扩展的DeepSeek-GRM-27B的性能，已经达到了与强大的闭源模型GPT-4o以及参数量为其25倍的DeepSeek-GRM-671B MoE模型相媲美的水平。这为“以计算换性能”的策略提供了强有力的实证支持。

### Analysis of Inference-Time Scalability

论文深入分析了性能随推理样本数量（$k$）增加的变化趋势。结果显示，SPCT训练的GRM模型展现出非常理想的扩展曲线。

- 与其他方法（如简单的LLM-as-a-Judge或标量RM）相比，DeepSeek-GRM的性能随着样本数的增加，呈现出更陡峭、更持续的增长。这表明SPCT确实成功地在模型中植入了“可扩展的行为”，使其能够有效利用更多的计算资源来深化其“思考”，从而做出更准确的判断。
- Meta RM引导的投票机制被证明是有效的。它能够在较少的样本数下（例如 $k=8$）就达到接近饱和的性能，从而在性能和计算成本之间提供了一个更优的平衡点。

### Ablation Studies: Deconstructing the Source of Performance

为了探究SPCT方法中各个组件的真实贡献，论文进行了一系列详尽的消融研究：

- **原则生成的重要性**: 实验发现，如果在提示中移除生成“原则”的要求，模型的性能会出现显著下降。这证明了“原则”不仅仅是装饰性的文本，而是模型进行高质量、泛化判断的核心认知框架。
- **在线RL的价值**: 即使去掉第一阶段的RFT（冷启动），仅使用通用指令数据预训练的模型在经过第二阶段的在线RL后，性能依然有巨大提升。这突显了在线RL在精炼模型判断能力、使其超越简单模仿方面的关键作用。
- **Meta RM的鲁棒性**: 实验表明，Meta RM引导的投票机制在不同的过滤阈值（$k_{meta}$）下都表现出稳健的性能提升，证实了其作为质量过滤器的有效性和实用性。

---

## 💬 Personal Insights

这篇论文不仅提出了一套行之有效的技术方案，更在深层次上为我们思考AI对齐和能力扩展提供了新的视角。其核心思想为我们实践探索提供了理论指导，并最终被我们的实验成功所验证，形成了一个从理论到实践的闭环。

### The Core Innovation: Shifting from Opaque Scoring to Interpretable Reasoning

这篇论文最深刻的贡献，或许并非单纯的技术指标提升，而是一种**认知论层面（epistemological）的范式转移**。它将奖励建模从一个旨在输出不透明、不可解释的**标量分数**的任务，转变为一个旨在生成明确、可验证的**推理链条**（即原则和批判）的任务。

这一转变带来了深远的影响：

- **可解释性与可调试性**：当一个传统RM犯错时，我们很难知道其内部的“症结”所在。而对于GRM，我们可以直接检查其生成的批判文本，定位到导致错误判断的那个有问题的“原则”或“逻辑谬误”。
- **对齐的深化**：它推动我们从对齐模型的“行为”（输出正确的偏好）深化到对齐模型的“思考过程”（基于我们认可的原则进行判断）。这是一种更根本、更鲁棒的对齐范式。

### Our Practice: A Validation and Application of the GRM Philosophy

在阅读了这篇论文并受到其GRM思想的启发后，我们设计并执行了一条清晰的实践路线，旨在验证并应用其核心理念。我们的工作流程不仅成功复现了论文方法的有效性，还将其思想扩展到了数据质量的持续改进中。

1. **Step 1: Validating the "Principle" Hypothesis.**
   我们的第一步是验证论文的核心假设：**“原则”是否是指导LLM进行高质量判断的关键**。我们通过线上数据聚类，挑选出20个好案例和20个坏案例，让DeepSeek和Doubao从中总结出区分优劣的核心原则。随后，我们将这些提炼出的原则加入到提示工程（PE）中，发现模型的判断准确率确实得到了显著提升。并且直接使用PE+LLM当做reward model验证了筛选sft数据的有效性。

2. **Step 2: Building Our GRM via RFT + RL.**
   在验证了原则的有效性之后，我们开始着手构建自己的生成式奖励模型。我们严格遵循了论文提出的SPCT两阶段训练法：首先，利用我们提炼出的原则作为高质量标准来筛选数据，进行**拒绝式微调（RFT）**，教会模型生成结构化的批判文本；接着，我们采用**在线强化学习（RL, GRPO）** 对模型进行优化，使其生成的批判能更准确地对回复进行排序。最终，我们成功训练出了一个表现出高度一致性的GRM。

3. **Step 3: Creating a Data Improvement Flywheel.**
   我们并未止步于训练出一个静态的GRM。更进一步，我们将其整合到了我们的数据生产流程中，创造了一个持续优化的“飞轮”。我们使用训练好的GRM对新的SFT（监督微调）数据进行**持续的拒绝采样**。GRM会评估新的训练样本与线上模型产生的样本好坏，并自动拒绝那些质量不符合其内部原则的数据。这个过程极大地提升了我们SFT数据的整体质量，从而训练出更强大的基础模型，形成了一个正向循环。

我们的实践路径，从验证“原则”的有效性，到复现RFT+RL的训练流程，再到将GRM应用于SFT数据质量的持续提升。

### Potential Limitations and Future Directions

尽管该方法取得了巨大成功，但仍存在一些值得探讨的局限和未来方向：

- **计算开销**：推理时扩展的成本是显著的，例如32次采样的计算成本是单次前向传播的32倍。对于延迟敏感的在线应用，如何在性能和成本之间取得平衡是一个需要仔细研究的工程问题。
- **原则的偏见与被利用风险**：模型生成的原则并非绝对正确或普适。模型可能会学会生成一些听起来合理但实际上有偏见或存在逻辑漏洞的原则，从而“欺骗”奖励系统，这是一种更高级的“奖励骇客”（reward hacking）形式。
- **未来方向**：
  - **原则的验证与修正**：未来的研究可以探索如何将外部知识库或逻辑验证工具整合进来，对模型生成的原则进行事实性和逻辑性校验。
  - **更高效的扩展策略**：研究更先进的采样或推理技术，例如动态调整样本数量或使用推测性解码（speculative decoding）等方法，以降低扩展的计算成本。
  - **与在线智能体的结合**：将这种能够自我批判和生成原则的奖励模型，整合到在线的、与环境交互的强化学习智能体（agent）训练流程中，可能会催生出更具反思和规划能力的AI系统 。

---

## 🔗 Related Papers

- [dapo](./20250318_dapo.md)
