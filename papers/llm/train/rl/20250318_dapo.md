---
title: 'DAPO: An Open-Source LLM Reinforcement Learning System at Scale'
date: 2025-08-17T00:00:00.000Z
tags:
  - rlhf
  - grpo
category: paper
venue: arXiv
authors: zyz
---
# DAPO

## 📖 Paper Info

- **Title:** DAPO: An Open-Source LLM Reinforcement Learning System at Scale
- **Authors:** ByteDance Seed
- **Venue:** arXiv
- **Paper Link:** [https://arxiv.org/pdf/2503.14476](https://arxiv.org/pdf/2503.14476)
- **Project Page:** [https://dapo-sia.github.io/](https://dapo-sia.github.io/)
- **Publication Date:** 2025.3.18

---

## 📝 Summary

作者团队提出了DAPO（Decoupled Clip and Dynamic sAmpling Policy Optimization，解耦裁剪与动态采样策略优化）算法，并围绕该算法构建了一个完全开源的大规模LLM强化学习系统。

DAPO算法旨在克服在长思维链（long Chain-of-Thought, CoT）推理任务中进行RL训练时常见的技术瓶颈，例如策略熵崩溃、奖励噪声和训练不稳定性。该项目不仅是发布一个算法，而是提供了一个完整的、可复现的解决方案，包括基于`verl`框架的训练代码、一个经过精心处理和标注的数学推理数据集（DAPO-Math-17k），以及最终训练好的模型权重。

使用DAPO算法对Qwen2.5-32B基础模型进行训练，在极具挑战性的AIME 2024数学竞赛基准测试中取得了50分的优异成绩。这一结果不仅超越了之前由DeepSeek-R1-Zero-Qwen-32B保持的47分的最高纪录，而且仅用了其一半的训练步数就达到了这一性能。

---

## 🔑 Key Contributions

该论文的核心贡献在于提出了DAPO算法，它由四个相互关联的创新技术组成，专门用于解决在长思维链（long-CoT）强化学习场景中遇到的具体挑战。

1. **Clip-Higher (更高上限裁剪):** 这是对PPO风格裁剪目标函数的一项关键改进。传统PPO使用对称的裁剪范围（例如，$[1-\epsilon, 1+\epsilon]$）来限制策略更新的幅度。然而，在长CoT任务中，这种对称限制可能导致“熵崩溃”——即模型策略变得过于确定，丧失探索新推理路径的能力。DAPO通过解耦上下裁剪边界，允许一个更高的上界（$1+\epsilon_{high}$），从而为那些概率较低但可能有效的探索性Token提供更强的正向激励。

2. **Dynamic Sampling (动态采样):** 在RL训练中，如果一个批次（batch）内的所有样本都得到完全正确或完全错误的奖励，那么这个批次提供的梯度信息量就非常有限，甚至可能产生误导。动态采样策略通过对每个问题（prompt）进行超量采样，然后过滤掉那些所有生成答案准确率恒为1或0的问题组。这样可以确保每个用于训练的批次都包含具有信息量差异的样本，从而产生有效的梯度信号，减少了梯度方差，并避免了在无效数据上的计算资源浪费。

3. **Token-Level Policy Gradient Loss (Token级策略梯度损失):** 在推理任务中，答案的长度往往与推理过程的复杂性正相关。样本级的损失函数对每个答案（不论长短）赋予相同的权重，这会稀释长而复杂的正确推理路径所包含的宝贵信号。DAPO改为采用Token级的损失加权，即每个样本对总损失的贡献与其生成的Token数量成正比。这确保了更长的、包含详细推理步骤的优质答案在梯度更新中占有更大的比重，从而更有效地学习复杂的多步推理模式。

4. **Overlong Reward Shaping (超长奖励修正):** 当模型生成的答案超过最大长度限制而被截断时，会产生两个问题：一是截断的序列可能提供不完整甚至错误的梯度信号；二是模型可能倾向于生成无意义的超长内容。该机制通过“超长过滤”（Overlong Filtering）屏蔽掉被截断样本的损失，防止噪声梯度；同时通过“软性超长惩罚”（Soft Overlong Punishment）对接近但未超过硬性长度限制的过长序列施加一个与长度相关的温和惩罚。

---

## 🧩 Method

DAPO方法论的构建是建立在对现有强化学习算法深刻理解的基础之上，并针对长CoT推理这一特定领域的痛点进行了系统性的创新。其设计不仅体现在算法层面，更贯穿于其核心哲学选择和数据工程的实践中。

### Foundational Concepts: From PPO to GRPO

现代LLM的强化学习实践大多源于**近端策略优化（Proximal Policy Optimization, PPO）**。PPO的核心思想是通过一个裁剪的代理目标函数（clipped surrogate objective）来限制每次策略更新的步长，确保新旧策略之间的差异不会过大，从而在提升样本效率的同时保证了训练的稳定性。

DAPO的直接技术基线是**组相对策略优化（Group Relative Policy Optimization, GRPO）**。GRPO是对PPO的一种重要改进，特别适用于大规模LLM训练。与PPO需要同时训练一个策略模型（Policy Model）和一个价值模型（Value Model）不同，GRPO通过在一个采样组（group）内对奖励进行归一化来相对地估计优势函数（Advantage），从而完全移除了价值模型。这一改变带来了显著的优势：它极大地降低了显存消耗。在需要生成数千乃至上万Token的长CoT推理任务中，显存是极其宝贵的资源。移除价值模型使得系统可以用更大的批次大小（batch size）进行训练，或者处理更长的序列。

### Core Philosophical Choices

DAPO的成功不仅源于其技术细节，也源于其在两个核心问题上的明确哲学立场：

- **Abandoning KL Divergence (放弃KL散度):** 在传统的RLHF（人类反馈强化学习）流程中，通常会加入一个KL散度惩罚项，其目的是约束当前策略$\pi_{\theta}$不要偏离初始的SFT（监督微调）模型$\pi_{ref}$太远，以防止模型遗忘通用能力或产生不符合人类偏好的内容。然而，DAPO的作者认为，在旨在激发深度推理能力的任务中，模型需要进行大幅度的探索，其最终的策略分布可能也应该与初始模型有显著差异。因此，他们果断地移除了KL散度惩罚项，为模型提供了更大的自由度去探索和学习全新的、更强大的推理模式。

- **Rule-Based Reward Modeling (基于规则的奖励建模):** 当前主流的RLHF依赖于训练一个独立的奖励模型（Reward Model）来模拟人类的偏好。然而，这种学习出来的奖励模型自身可能存在漏洞或偏见，导致策略模型学会“奖励骇客”（reward hacking）——即找到奖励模型的捷径以获得高分，但实际生成的内质量很差。DAPO通过选择具有可验证答案的数学任务，完全规避了这个问题。它直接使用最终答案的正确性作为奖励信号（正确为+1，错误为-1），这种基于规则的奖励是明确、稳定且无法被“欺骗”的。这一选择与DPO等方法所依赖的成对偏好数据（preference data）形成了鲜明对比。

### The DAPO Algorithm: A Detailed Breakdown

DAPO的整体目标函数可以表示为对GRPO的改进，其核心是加权和修正后的策略梯度：

$$
J_{\text{DAPO}}(\theta) = \mathbb{E}_{(q,a) \sim D, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G |o_i| \sum_{t=1}^{|o_i|} L_{\text{PG}}(o_{i,t}) \right]
$$

其中，$L_{\text{PG}}$是包含DAPO各项核心技术的策略梯度损失项。下面对四大核心技术进行详细拆解：

1. **Clip-Higher:** PPO的裁剪函数通常为$\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)$，其中$r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$是重要性采样比率。DAPO将其修改为$\text{clip}(r_t(\theta), 1-\epsilon_{\text{low}}, 1+\epsilon_{\text{high}})$，并设置$\epsilon_{\text{high}} > \epsilon_{\text{low}}$（例如，论文中设置为0.28 vs 0.2）。当模型探索到一个低概率但高回报的动作（token）时，$r_t(\theta)$会很大，非对称裁剪允许其获得更大的梯度更新信号（最高可达$1+\epsilon_{\text{high}}$），从而更积极地鼓励这种探索行为，直接对抗熵崩溃。![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171523225.png)![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171524375.png)

2. **Dynamic Sampling:** 该过程的伪代码逻辑如下：
   - 对于每个prompt $p$，采样一个超额的响应集合$S' = \{o_1, o_2,..., o_{G'}\}$，其中$G' > G$。
   - 对$S'$中的每个响应$o_i$计算其准确率$acc(o_i) \in \{0, 1\}$。
   - 如果$\forall o_i \in S', acc(o_i) = 1$或者$\forall o_i \in S', acc(o_i) = 0$，则丢弃该prompt $p$及其所有响应。
   - 否则，保留该prompt组用于构建训练批次。
     这个过程确保了训练数据中始终包含“有争议”的样本，即模型在该prompt上既能生成正确答案也能生成错误答案，这样的数据才能提供最有效的学习信号，从而稳定梯度并加速收敛。
   ![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171527527.png)


2. **Token-Level Policy Gradient Loss:** 从目标函数可以看出，每个样本的损失$L_{\text{PG}}$被其Token长度$|o_i|$加权。这意味着，一个包含1000个Token的详细推理过程对最终梯度的贡献是一个只包含100个Token的简短回答的10倍。这种机制使得模型更加关注那些能够产出长而连贯的、结构复杂的正确推理链的策略，这对于学习深度推理至关重要。![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171525280.png)


3. **Overlong Reward Shaping:** 这个机制包含两个部分：
   - **Overlong Filtering:** 如果一个样本因为达到最大生成长度而被截断，那么该样本的损失在计算时将被乘以0，即完全忽略。这可以防止模型从一个不完整的、可能具有误导性的推理结尾中学习到错误的模式。
   - **Soft Overlong Punishment:** 作者定义了一个“软惩罚”区间，例如最大期望长度为16384个Token，而最大生成长度为20480个Token。当生成的序列长度落入(16384, 20480]这个区间并被截断时，系统会施加一个与长度相关的惩罚性奖励。这向模型传递了一个清晰的信号：避免生成冗长但没有结果的序列。![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171526118.png)

![image.png](https://raw.githubusercontent.com/kyriekevin/img_auto/main/Obsidian/202508171526385.png)

### Data Engineering: The DAPO-Math-17K Dataset

DAPO系统的另一个基石是其高质量的训练数据。为了给基于规则的奖励机制提供干净、无歧义的信号，作者团队设计了一套创新的“答案转换”流程。他们利用LLM本身，将数学问题进行改写，使得原始问题中复杂的答案形式（如分数$\frac{a}{b}$、根式$a+\sqrt{b}$等）被转化为一个简单的整数（例如，改写问题，要求回答$a+b$）。这个过程通常包含四个步骤：提取原始答案格式、重写问题描述、解决新问题、并提供一个整数作为最终答案。通过这种方式，他们构建了DAPO-Math-17K数据集，其中每个问题都对应一个唯一的整数答案，从而为强化学习提供了完美、可靠的奖励来源。

---

## 📊 Experiments & Results

详细介绍了DAPO算法的实验验证过程，包括实验设置、关键的量化结果以及对训练动态的深入分析，共同构成了对其有效性的有力证明。

### Experimental Setup

- **Model:** 实验选用Qwen2.5-32B作为基础模型，这是一个强大的开源语言模型，为后续的RL训练提供了坚实的基础。
- **Framework:** 训练过程基于`verl`框架实现，这是一个为大规模RL设计的系统。
- **Hardware:** 整个训练在由128块H20 GPU组成的集群上进行。
- **Dataset:** 训练集使用专门构建的DAPO-Math-17K，验证集则采用AIME 2024竞赛题目。
- **Key Hyperparameters:** 优化器采用AdamW，学习率恒定为$1 \times 10^{-6}$，并设置了20个rollout步骤的线性预热。Prompt的批次大小为512，每个prompt采样16个响应。对于Clip-Higher技术，设置$\epsilon_{\text{low}}=0.2$和$\epsilon_{\text{high}}=0.28$ 。
- **Evaluation Metric:** 为了获得稳定可靠的评估结果，在AIME 2024验证集上的评估重复进行32次，并报告平均准确率（avg@32）。

### Main Results and Ablation Study

实验的核心结果：基于Qwen2.5-32B的DAPO模型在AIME 2024测试中取得了50%的准确率。这一成绩不仅超越了之前由DeepSeek-R1-Zero-Qwen-32B在该模型上取得的47%的SOTA结果，而且实现这一性能所用的训练步数减少了50%。

为了系统地验证DAPO各组件的贡献，论文进行了一项详细的消融研究。这项研究是论文中最关键的证据，它将作者的理论主张转化为经验上可验证的事实，并证明了DAPO相对于简单GRPO基线的复杂性是合理且必要的。

| **方法**                           | **AIME 2024 准确率 (%)** | **增量收益** |
| :--------------------------------- | :----------------------: | :----------: |
| Naive GRPO (基线)                  |            30            |      -       |
| + Overlong Filtering               |            36            |      +6      |
| + Clip-Higher                      |            38            |      +2      |
| + Soft Overlong Punishment         |            41            |      +3      |
| + Token-level Loss                 |            42            |      +1      |
| + Dynamic Sampling **(完整 DAPO)** |          **50**          |    **+8**    |

从表中可以清晰地看到，DAPO的每个组件都对最终性能做出了正向贡献。其中，Overlong Filtering和Dynamic Sampling带来了最显著的性能提升，分别贡献了6%和8%的准确率增长，凸显了处理长序列噪声和优化梯度质量的重要性。

### Analysis of Training Dynamics

论文还对训练过程中的关键中间指标进行了监控和分析，揭示了大规模LLM RL训练的复杂性。

- **Response Length (响应长度):** 实验观察到，生成响应的平均长度与模型的探索能力和最终性能密切相关。长度的稳步增长通常意味着模型正在探索更复杂的推理空间。反之，长度的停滞甚至下降，则可能是训练恶化的一个信号 。

- **Reward (奖励):** 训练过程中的奖励值通常会稳定上升，表明模型在拟合训练集数据。然而，一个非常重要的发现是，模型在训练集上的最终奖励值与它在验证集上的准确率之间几乎没有相关性。作者将此现象归因于模型对训练集的**过拟合**。这一发现揭示了一个深刻的挑战：即使有完美、基于规则的奖励信号，模型也可能不是在学习一种可泛化的“推理”能力，而是在记忆训练集中特定问题的解题模式或统计规律。这使得RL“解锁”模型潜能的说法变得复杂，并暗示DAPO的各项技术（如维持熵的Clip-Higher和提升梯度多样性的Dynamic Sampling）可能起到了隐式正则化的作用，防止了更严重的泛化能力崩溃。

- **Model Entropy (模型熵):** 熵是衡量策略探索能力的关键指标。论文中的图表显示，朴素的GRPO基线在训练过程中遭遇了严重的熵崩溃，而Clip-Higher技术有效地缓解了这一问题，维持了策略的探索性，这对性能的持续提升至关重要 。

### Qualitative Findings: Emergent Behaviors

除了量化结果，论文还报告了一个有趣的定性发现：在训练过程中，模型自发地涌现出了更高级的推理行为。例如，在训练初期模型不具备的**自我反思（self-reflection）** 和 **回溯（backtracking）** 等复杂策略，随着训练的进行而逐渐出现。

---

## 💬 Personal Insights

从算法设计的深层视角审视DAPO，其核心在于对策略熵（Exploration）和优势信号（Exploitation）之间平衡的精妙调控。DAPO的各项技术组件为解决这一经典难题提供了有效的工程化方案，但同时也为更精细化的未来探索留下了广阔空间。基于DAPO的框架，可以进一步构想和实践一系列更具适应性的高级训练策略。

### 核心机制的再理解：熵与优势样本组

- **熵的管理哲学：** DAPO通过`Clip-Higher`技术直接对抗熵崩溃，这是一种全局性的、略显“粗暴”但有效的干预手段，它为模型保留了探索未知解题路径的可能性 。然而，理想的熵管理不应是全局性的，而应是与任务难度动态相关的。在训练初期面对简单问题时，我们期望模型能快速收敛，形成稳定、低熵的策略；而在后期面对复杂难题时，则需要更高的熵来鼓励探索。

- **优势样本组的构建：** `Dynamic Sampling`是DAPO的精髓所在，其本质是确保每个训练批次（batch）都包含有效的“优势信号”——即组内既有正奖励样本也有负奖励样本，从而产生有意义的梯度 。这解决了梯度方差过大的问题。然而，这种“有或无”的过滤方式（即完全丢弃全对或全错的组）可能过于绝对，忽略了样本组内部的“矛盾”程度，也未考虑模型在不同学习阶段对样本的需求变化。

### 基于DAPO的进阶策略与实验性技巧

结合DAPO的基础，以下三种进阶策略旨在将训练过程从静态规则驱动，转变为一个更动态、更智能的数据驱动过程：
1. **课程学习（Curriculum Learning）：** DAPO采用的是对整个数据集的随机采样，但这可能不是最高效的学习路径。引入课程学习，可以让模型“由简到难”地学习，从而构建更扎实的推理能力。
    - **单维度排序：** 最直接的方式是根据模型在各个问题（prompt）上的历史**通过率**进行排序。训练初期，优先选择通过率高（简单）的样本，让模型快速掌握基础模式。

    - **多维度排序与“矛盾”样本：** 更精细化的方法是结合**通过率**和**方差**。这里的方差可以理解为：对于一个包含多个子维度或解题步骤的问题，如果模型在这些子维度上的得分高低不齐，说明这是一个“矛盾”的样本，其标准差会较大。反之，如果得分一致（全对或全错），标准差则为零。课程可以设计为从“信号一致”（低方差）的样本过渡到“信号矛盾”（高方差）的样本，引导模型逐步解决更具挑战性的问题。

2. **作弊采样（Cheat Sampling）：** DAPO的动态采样会过滤掉通过率为0的样本组，但这可能导致模型在某些难题上永远无法取得突破，因为模型自身根本无法生成一个正确的答案来提供正向信号。
    - **注入正确样本：** “作弊采样”机制旨在解决这个问题。当一个样本组的通过率为0时，我们可以强制性地将一个由人类专家或更强模型生成的正确答案（Ground Truth, GT）注入到这个组中。
    - **提供非零优势与高熵引导：** 这样做有两个关键作用：首先，它确保了组内存在一个正奖励样本，从而产生一个宝贵的、非零的优势信号，指导模型如何在难题上进行学习。其次，对于模型难以解决的问题，其正确答案（GT）的生成路径通常是模型当前策略下的低概率事件，即具有高熵。注入这样的高熵样本，特别是在课程学习的后期，可以有效提升模型在面对hard case时的平均熵，增加探索，避免因过度拟合简单模式而导致的熵坍塌。

3. **SSR机制（Significant Sample Replay）：** 对于标准差为0的样本组（例如，所有采样答案都错误），即使不过滤，其提供的学习信号也十分有限。SSR机制旨在提升这类低效样本组的价值。
    - **替换与重放：** SSR的核心思想是，当遇到一个低价值（如std=0）的样本组时，用一个当前存储的、具有高学习价值的“显著样本组”来替换它进行训练。
    - **高价值样本的定义：** “显著样本”可以是那些模型表现出“反常”行为的组，例如“难题做对”（表明模型探索到了新的有效路径）或“简单题做错”（表明模型在基础能力上存在漏洞）。通过作弊采样机制注入的GT样本组，天然就是一种高价值的显著样本。通过重放这些关键样本，可以强化模型对关键知识点的学习和记忆，从而更高效地进行优化。


---

## 🔗 Related Papers
